{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "import cn2an\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import keras\n",
    "# import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Multiply, Masking, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras_bert import get_checkpoint_paths, load_vocabulary, Tokenizer, load_trained_model_from_checkpoint\n",
    "\n",
    "from nl2sql.utils.optimizer import RAdam\n",
    "from nl2sql.utils import read_data, read_tables, SQL, MultiSentenceTokenizer, Query, Question, Table, read_single_data\n",
    "\n",
    "# config\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'   \n",
    "NUM_GPUS = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_brackets(s):\n",
    "    '''\n",
    "    Remove brackets [] () from text\n",
    "    '''\n",
    "    return re.sub(r'[\\(\\（].*[\\)\\）]', '', s)\n",
    "\n",
    "class QueryTokenizer(MultiSentenceTokenizer):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize query (question + table header) and encode to integer sequence.\n",
    "    Using reserved tokens [unused11] and [unused12] for classification\n",
    "    \"\"\"\n",
    "    \n",
    "    col_type_token_dict = {'text': '[unused11]', 'real': '[unused12]'}\n",
    "    \n",
    "    def tokenize(self, query: Query, col_orders=None):\n",
    "        \"\"\"\n",
    "        Tokenize quesiton and columns and concatenate.\n",
    "        \n",
    "        Parameters:\n",
    "        query (Query): A query object contains question and table\n",
    "        col_orders (list or numpy.array): For re-ordering the header columns\n",
    "        \n",
    "        Returns:\n",
    "        token_idss: token ids for bert encoder\n",
    "        segment_ids: segment ids for bert encoder\n",
    "        header_ids: positions of columns\n",
    "        \"\"\"\n",
    "        \n",
    "        question_tokens = [self._token_cls] + self._tokenize(query.question.text)\n",
    "        header_tokens = []\n",
    "        \n",
    "        if col_orders is None:\n",
    "            col_orders = np.arange(len(query.table.header))\n",
    "        \n",
    "        header = [query.table.header[i] for i in col_orders]\n",
    "        \n",
    "        for col_name, col_type in header:\n",
    "            col_type_token = self.col_type_token_dict[col_type]\n",
    "            col_name = remove_brackets(col_name)\n",
    "            col_name_tokens = self._tokenize(col_name)\n",
    "            col_tokens = [col_type_token] + col_name_tokens\n",
    "            header_tokens.append(col_tokens)\n",
    "            \n",
    "        all_tokens = [question_tokens] + header_tokens\n",
    "        return self._pack(*all_tokens)\n",
    "    \n",
    "    def encode(self, query:Query, col_orders=None):\n",
    "        tokens, tokens_lens = self.tokenize(query, col_orders)\n",
    "        token_ids = self._convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        header_indices = np.cumsum(tokens_lens)\n",
    "        return token_ids, segment_ids, header_indices[:-1]\n",
    "\n",
    "class SqlLabelEncoder:\n",
    "    \"\"\"\n",
    "    Convert SQL object into training labels.\n",
    "    \"\"\"\n",
    "    def encode(self, sql: SQL, num_cols):\n",
    "        cond_conn_op_label = sql.cond_conn_op\n",
    "        \n",
    "        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n",
    "        for col_id, agg_op in zip(sql.sel, sql.agg):\n",
    "            if col_id < num_cols:\n",
    "                sel_agg_label[col_id] = agg_op\n",
    "            \n",
    "        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n",
    "        for col_id, cond_op, _ in sql.conds:\n",
    "            if col_id < num_cols:\n",
    "                cond_op_label[col_id] = cond_op\n",
    "            \n",
    "        return cond_conn_op_label, sel_agg_label, cond_op_label\n",
    "    \n",
    "    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n",
    "        cond_conn_op = int(cond_conn_op_label)\n",
    "        sel, agg, conds = [], [], []\n",
    "\n",
    "        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n",
    "            if agg_op < len(SQL.agg_sql_dict):\n",
    "                sel.append(col_id)\n",
    "                agg.append(int(agg_op))\n",
    "            if cond_op < len(SQL.op_sql_dict):\n",
    "                conds.append([col_id, int(cond_op)])\n",
    "        return {\n",
    "            'sel': sel,\n",
    "            'agg': agg,\n",
    "            'cond_conn_op': cond_conn_op,\n",
    "            'conds': conds\n",
    "        }\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Generate training data in batches\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 tokenizer, \n",
    "                 label_encoder, \n",
    "                 is_train=True, \n",
    "                 max_len=160, \n",
    "                 batch_size=32, \n",
    "                 shuffle=True, \n",
    "                 shuffle_header=True, \n",
    "                 global_indices=None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_header = shuffle_header\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if global_indices is None:\n",
    "            self._global_indices = np.arange(len(data))\n",
    "        else:\n",
    "            self._global_indices = global_indices\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "    \n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        padded = pad_sequences(seqs, maxlen=None, padding='post', truncating='post')\n",
    "        if max_len is not None:\n",
    "            padded = padded[:, :max_len]\n",
    "        return padded\n",
    "    \n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices = \\\n",
    "            self._global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "        \n",
    "        TOKEN_IDS, SEGMENT_IDS = [], []\n",
    "        HEADER_IDS, HEADER_MASK = [], []\n",
    "        \n",
    "        COND_CONN_OP = []\n",
    "        SEL_AGG = []\n",
    "        COND_OP = []\n",
    "        \n",
    "        for query in batch_data:\n",
    "            question = query.question.text\n",
    "            table = query.table\n",
    "            \n",
    "            col_orders = np.arange(len(table.header))\n",
    "            if self.shuffle_header:\n",
    "                np.random.shuffle(col_orders)\n",
    "            \n",
    "            token_ids, segment_ids, header_ids = self.tokenizer.encode(query, col_orders)\n",
    "            header_ids = [hid for hid in header_ids if hid < self.max_len]\n",
    "            header_mask = [1] * len(header_ids)\n",
    "            col_orders = col_orders[: len(header_ids)]\n",
    "            \n",
    "            TOKEN_IDS.append(token_ids)\n",
    "            SEGMENT_IDS.append(segment_ids)\n",
    "            HEADER_IDS.append(header_ids)\n",
    "            HEADER_MASK.append(header_mask)\n",
    "            \n",
    "            if not self.is_train:\n",
    "                continue\n",
    "            sql = query.sql\n",
    "            \n",
    "            cond_conn_op, sel_agg, cond_op = self.label_encoder.encode(sql, num_cols=len(table.header))\n",
    "            \n",
    "            sel_agg = sel_agg[col_orders]\n",
    "            cond_op = cond_op[col_orders]\n",
    "            \n",
    "            COND_CONN_OP.append(cond_conn_op)\n",
    "            SEL_AGG.append(sel_agg)\n",
    "            COND_OP.append(cond_op)\n",
    "            \n",
    "        TOKEN_IDS = self._pad_sequences(TOKEN_IDS, max_len=self.max_len)\n",
    "        SEGMENT_IDS = self._pad_sequences(SEGMENT_IDS, max_len=self.max_len)\n",
    "        HEADER_IDS = self._pad_sequences(HEADER_IDS)\n",
    "        HEADER_MASK = self._pad_sequences(HEADER_MASK)\n",
    "        \n",
    "        inputs = {\n",
    "            'input_token_ids': TOKEN_IDS,\n",
    "            'input_segment_ids': SEGMENT_IDS,\n",
    "            'input_header_ids': HEADER_IDS,\n",
    "            'input_header_mask': HEADER_MASK\n",
    "        }\n",
    "        \n",
    "        if self.is_train:\n",
    "            SEL_AGG = self._pad_sequences(SEL_AGG)\n",
    "            SEL_AGG = np.expand_dims(SEL_AGG, axis=-1)\n",
    "            COND_CONN_OP = np.expand_dims(COND_CONN_OP, axis=-1)\n",
    "            COND_OP = self._pad_sequences(COND_OP)\n",
    "            COND_OP = np.expand_dims(COND_OP, axis=-1)\n",
    "\n",
    "            outputs = {\n",
    "                'output_sel_agg': SEL_AGG,\n",
    "                'output_cond_conn_op': COND_CONN_OP,\n",
    "                'output_cond_op': COND_OP\n",
    "            }\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "\n",
    "def seq_gather(x):\n",
    "    seq, idxs = x\n",
    "    idxs = K.cast(idxs, 'int32')\n",
    "    return K.tf.batch_gather(seq, idxs)\n",
    "\n",
    "def outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, header_lens, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate sqls from model outputs\n",
    "    \"\"\"\n",
    "    preds_cond_conn_op = np.argmax(preds_cond_conn_op, axis=-1)\n",
    "    preds_cond_op = np.argmax(preds_cond_op, axis=-1)\n",
    "\n",
    "    sqls = []\n",
    "    \n",
    "    for cond_conn_op, sel_agg, cond_op, header_len in zip(preds_cond_conn_op, \n",
    "                                                          preds_sel_agg, \n",
    "                                                          preds_cond_op, \n",
    "                                                          header_lens):\n",
    "        sel_agg = sel_agg[:header_len]\n",
    "        # force to select at least one column for agg\n",
    "        sel_agg[sel_agg == sel_agg[:, :-1].max()] = 1\n",
    "        sel_agg = np.argmax(sel_agg, axis=-1)\n",
    "        \n",
    "        sql = label_encoder.decode(cond_conn_op, sel_agg, cond_op)\n",
    "        sql['conds'] = [cond for cond in sql['conds'] if cond[0] < header_len]\n",
    "        \n",
    "        sel = []\n",
    "        agg = []\n",
    "        for col_id, agg_op in zip(sql['sel'], sql['agg']):\n",
    "            if col_id < header_len:\n",
    "                sel.append(col_id)\n",
    "                agg.append(agg_op)\n",
    "                \n",
    "        sql['sel'] = sel\n",
    "        sql['agg'] = agg\n",
    "        sqls.append(sql)\n",
    "    return sqls\n",
    "\n",
    "class EvaluateCallback(Callback):\n",
    "    def __init__(self, val_dataseq):\n",
    "        self.val_dataseq = val_dataseq\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred_sqls = []\n",
    "        for batch_data in self.val_dataseq:\n",
    "            header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n",
    "            preds_cond_conn_op, preds_sel_agg, preds_cond_op = self.model.predict_on_batch(batch_data)\n",
    "            sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n",
    "                                   header_lens, self.val_dataseq.label_encoder)\n",
    "            pred_sqls += sqls\n",
    "            \n",
    "        conn_correct = 0\n",
    "        agg_correct = 0\n",
    "        conds_correct = 0\n",
    "        conds_col_id_correct = 0\n",
    "        all_correct = 0\n",
    "        num_queries = len(self.val_dataseq.data)\n",
    "        \n",
    "        true_sqls = [query.sql for query in self.val_dataseq.data]\n",
    "        for pred_sql, true_sql in zip(pred_sqls, true_sqls):\n",
    "            n_correct = 0\n",
    "            if pred_sql['cond_conn_op'] == true_sql.cond_conn_op:\n",
    "                conn_correct += 1\n",
    "                n_correct += 1\n",
    "            \n",
    "            pred_aggs = set(zip(pred_sql['sel'], pred_sql['agg']))\n",
    "            true_aggs = set(zip(true_sql.sel, true_sql.agg))\n",
    "            if pred_aggs == true_aggs:\n",
    "                agg_correct += 1\n",
    "                n_correct += 1\n",
    "\n",
    "            pred_conds = set([(cond[0], cond[1]) for cond in pred_sql['conds']])\n",
    "            true_conds = set([(cond[0], cond[1]) for cond in true_sql.conds])\n",
    "\n",
    "            if pred_conds == true_conds:\n",
    "                conds_correct += 1\n",
    "                n_correct += 1\n",
    "   \n",
    "            pred_conds_col_ids = set([cond[0] for cond in pred_sql['conds']])\n",
    "            true_conds_col_ids = set([cond[0] for cond in true_sql['conds']])\n",
    "            if pred_conds_col_ids == true_conds_col_ids:\n",
    "                conds_col_id_correct += 1\n",
    "            \n",
    "            if n_correct == 3:\n",
    "                all_correct += 1\n",
    "\n",
    "        print('conn_acc: {}'.format(conn_correct / num_queries))\n",
    "        print('agg_acc: {}'.format(agg_correct / num_queries))\n",
    "        print('conds_acc: {}'.format(conds_correct / num_queries))\n",
    "        print('conds_col_id_acc: {}'.format(conds_col_id_correct / num_queries))\n",
    "        print('total_acc: {}'.format(all_correct / num_queries))\n",
    "        \n",
    "        logs['val_tot_acc'] = all_correct / num_queries\n",
    "        logs['conn_acc'] = conn_correct / num_queries\n",
    "        logs['conds_acc'] = conds_correct / num_queries\n",
    "        logs['conds_col_id_acc'] = conds_col_id_correct / num_queries\n",
    "\n",
    "\n",
    "def get_model(num_sel_agg,num_cond_op,num_cond_conn_op, paths):\n",
    "\n",
    "    bert_model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=None)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "        \n",
    "    inp_token_ids = Input(shape=(None,), name='input_token_ids', dtype='int32')\n",
    "    inp_segment_ids = Input(shape=(None,), name='input_segment_ids', dtype='int32')\n",
    "    inp_header_ids = Input(shape=(None,), name='input_header_ids', dtype='int32')\n",
    "    inp_header_mask = Input(shape=(None, ), name='input_header_mask')\n",
    "\n",
    "    x = bert_model([inp_token_ids, inp_segment_ids]) # (None, seq_len, 768)\n",
    "\n",
    "    # predict cond_conn_op\n",
    "    x_for_cond_conn_op = Lambda(lambda x: x[:, 0])(x) # (None, 768)\n",
    "    p_cond_conn_op = Dense(num_cond_conn_op, activation='softmax', name='output_cond_conn_op')(x_for_cond_conn_op)\n",
    "\n",
    "    # predict sel_agg\n",
    "    x_for_header = Lambda(seq_gather, name='header_seq_gather')([x, inp_header_ids]) # (None, h_len, 768)\n",
    "    header_mask = Lambda(lambda x: K.expand_dims(x, axis=-1))(inp_header_mask) # (None, h_len, 1)\n",
    "\n",
    "    x_for_header = Multiply()([x_for_header, header_mask])\n",
    "    x_for_header = Masking()(x_for_header)\n",
    "\n",
    "    p_sel_agg = Dense(num_sel_agg, activation='softmax', name='output_sel_agg')(x_for_header)\n",
    "\n",
    "    x_for_cond_op = Concatenate(axis=-1)([x_for_header, p_sel_agg])\n",
    "    p_cond_op = Dense(num_cond_op, activation='softmax', name='output_cond_op')(x_for_cond_op)\n",
    "\n",
    "    model = Model(\n",
    "        [inp_token_ids, inp_segment_ids, inp_header_ids, inp_header_mask],\n",
    "        [p_cond_conn_op, p_sel_agg, p_cond_op]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def cn_to_an(string):\n",
    "    try:\n",
    "        return str(cn2an.cn2an(string, 'normal'))\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def an_to_cn(string):\n",
    "    try:\n",
    "        return str(cn2an.an2cn(string))\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def str_to_num(string):\n",
    "    try:\n",
    "        float_val = float(cn_to_an(string))\n",
    "        if int(float_val) == float_val:   \n",
    "            return str(int(float_val))\n",
    "        else:\n",
    "            return str(float_val)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def str_to_year(string):\n",
    "    year = string.replace('年', '')\n",
    "    year = cn_to_an(year)\n",
    "    if is_float(year) and float(year) < 1900:\n",
    "        year = int(year) + 2000\n",
    "        return str(year)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def load_json(json_file):\n",
    "    result = []\n",
    "    if json_file:\n",
    "        with open(json_file) as file:\n",
    "            for line in file:\n",
    "                result.append(json.loads(line))\n",
    "    return result\n",
    "\n",
    "class QuestionCondPair:\n",
    "    def __init__(self, query_id, question, cond_text, cond_sql, label):\n",
    "        self.query_id = query_id\n",
    "        self.question = question\n",
    "        self.cond_text = cond_text\n",
    "        self.cond_sql = cond_sql\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = ''\n",
    "        repr_str += 'query_id: {}\\n'.format(self.query_id)\n",
    "        repr_str += 'question: {}\\n'.format(self.question)\n",
    "        repr_str += 'cond_text: {}\\n'.format(self.cond_text)\n",
    "        repr_str += 'cond_sql: {}\\n'.format(self.cond_sql)\n",
    "        repr_str += 'label: {}\\n'.format(self.label)\n",
    "        return repr_str\n",
    "    \n",
    "class NegativeSampler:\n",
    "    \"\"\"\n",
    "    从 question - cond pairs 中采样\n",
    "    \"\"\"\n",
    "    def __init__(self, neg_sample_ratio=10):\n",
    "        self.neg_sample_ratio = neg_sample_ratio\n",
    "    \n",
    "    def sample(self, data):\n",
    "        positive_data = [d for d in data if d.label == 1]\n",
    "        negative_data = [d for d in data if d.label == 0]\n",
    "        negative_sample = random.sample(negative_data, \n",
    "                                        len(positive_data) * self.neg_sample_ratio)\n",
    "        return positive_data + negative_sample\n",
    "   \n",
    "class FullSampler:\n",
    "    \"\"\"\n",
    "    不抽样，返回所有的 pairs\n",
    "    \n",
    "    \"\"\"\n",
    "    def sample(self, data):\n",
    "        return data\n",
    "\n",
    "class CandidateCondsExtractor:\n",
    "    \"\"\"\n",
    "    params:\n",
    "        - share_candidates: 在同 table 同 column 中共享 real 型 candidates\n",
    "    \"\"\"\n",
    "    CN_NUM = '〇一二三四五六七八九零壹贰叁肆伍陆柒捌玖貮两'\n",
    "    CN_UNIT = '十拾百佰千仟万萬亿億兆点'\n",
    "    \n",
    "    def __init__(self, share_candidates=True):\n",
    "        self.share_candidates = share_candidates\n",
    "        self._cached = False\n",
    "    \n",
    "    def build_candidate_cache(self, queries):\n",
    "        self.cache = defaultdict(set)\n",
    "        print('building candidate cache')\n",
    "        for query_id, query in tqdm(enumerate(queries), total=len(queries)):\n",
    "            value_in_question = self.extract_values_from_text(query.question.text)\n",
    "            \n",
    "            for col_id, (col_name, col_type) in enumerate(query.table.header):\n",
    "                value_in_column = self.extract_values_from_column(query, col_id)\n",
    "                if col_type == 'text':\n",
    "                    cond_values = value_in_column\n",
    "                elif col_type == 'real':\n",
    "                    if len(value_in_column) == 1: \n",
    "                        cond_values = value_in_column + value_in_question\n",
    "                    else:\n",
    "                        cond_values = value_in_question\n",
    "                cache_key = self.get_cache_key(query_id, query, col_id)\n",
    "                self.cache[cache_key].update(cond_values)\n",
    "        self._cached = True\n",
    "    \n",
    "    def get_cache_key(self, query_id, query, col_id):\n",
    "        if self.share_candidates:\n",
    "            return (query.table.id, col_id)\n",
    "        else:\n",
    "            return (query_id, query.table.id, col_id)\n",
    "        \n",
    "    def extract_year_from_text(self, text):\n",
    "        values = []\n",
    "        num_year_texts = re.findall(r'[0-9][0-9]年', text)\n",
    "        values += ['20{}'.format(text[:-1]) for text in num_year_texts]\n",
    "        cn_year_texts = re.findall(r'[{}][{}]年'.format(self.CN_NUM, self.CN_NUM), text)\n",
    "        cn_year_values = [str_to_year(text) for text in cn_year_texts]\n",
    "        values += [value for value in cn_year_values if value is not None]\n",
    "        return values\n",
    "    \n",
    "    def extract_num_from_text(self, text):\n",
    "        values = []\n",
    "        num_values = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', text)\n",
    "        values += num_values\n",
    "        \n",
    "        cn_num_unit = self.CN_NUM + self.CN_UNIT\n",
    "        cn_num_texts = re.findall(r'[{}]*\\.?[{}]+'.format(cn_num_unit, cn_num_unit), text)\n",
    "        cn_num_values = [str_to_num(text) for text in cn_num_texts]\n",
    "        values += [value for value in cn_num_values if value is not None]\n",
    "    \n",
    "        cn_num_mix = re.findall(r'[0-9]*\\.?[{}]+'.format(self.CN_UNIT), text)\n",
    "        for word in cn_num_mix:\n",
    "            num = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', word)\n",
    "            for n in num:\n",
    "                word = word.replace(n, an_to_cn(n))\n",
    "            str_num = str_to_num(word)\n",
    "            if str_num is not None:\n",
    "                values.append(str_num)\n",
    "        return values\n",
    "    \n",
    "    def extract_values_from_text(self, text):\n",
    "        values = []\n",
    "        values += self.extract_year_from_text(text)\n",
    "        values += self.extract_num_from_text(text)\n",
    "        return list(set(values))\n",
    "   \n",
    "    def extract_values_from_column(self, query, col_ids):\n",
    "        question = query.question.text\n",
    "        question_chars = set(query.question.text)\n",
    "        unique_col_values = set(query.table.df.iloc[:, col_ids].astype(str))\n",
    "        select_col_values = [v for v in unique_col_values \n",
    "                             if (question_chars & set(v))]\n",
    "        return select_col_values\n",
    "      \n",
    "class QuestionCondPairsDataset:\n",
    "    \"\"\"\n",
    "    question - cond pairs 数据集\n",
    "    \"\"\"\n",
    "    OP_PATTERN = {\n",
    "        'real':\n",
    "        [\n",
    "            {'cond_op_idx': 0, 'pattern': '{col_name}大于{value}'},\n",
    "            {'cond_op_idx': 1, 'pattern': '{col_name}小于{value}'},\n",
    "            {'cond_op_idx': 2, 'pattern': '{col_name}是{value}'}\n",
    "        ],\n",
    "        'text':\n",
    "        [\n",
    "            {'cond_op_idx': 2, 'pattern': '{col_name}是{value}'}\n",
    "        ]\n",
    "    }    \n",
    "    \n",
    "    def __init__(self, queries, candidate_extractor, has_label=True, model_1_outputs=None):\n",
    "        self.candidate_extractor = candidate_extractor\n",
    "        self.has_label = has_label\n",
    "        self.model_1_outputs = model_1_outputs\n",
    "        self.data = self.build_dataset(queries)\n",
    "        \n",
    "    def build_dataset(self, queries):\n",
    "        if not self.candidate_extractor._cached:\n",
    "            self.candidate_extractor.build_candidate_cache(queries)\n",
    "            \n",
    "        pair_data = []\n",
    "        for query_id, query in enumerate(queries):\n",
    "            select_col_id = self.get_select_col_id(query_id, query)\n",
    "            for col_id, (col_name, col_type) in enumerate(query.table.header):\n",
    "                if col_id not in select_col_id:\n",
    "                    continue\n",
    "                    \n",
    "                cache_key = self.candidate_extractor.get_cache_key(query_id, query, col_id)\n",
    "                values = self.candidate_extractor.cache.get(cache_key, [])\n",
    "                pattern = self.OP_PATTERN.get(col_type, [])\n",
    "                pairs = self.generate_pairs(query_id, query, col_id, col_name, \n",
    "                                               values, pattern)\n",
    "                pair_data += pairs\n",
    "        return pair_data\n",
    "    \n",
    "    def get_select_col_id(self, query_id, query):\n",
    "        if self.model_1_outputs:\n",
    "            select_col_id = [cond_col for cond_col, *_ in self.model_1_outputs[query_id]['conds']]\n",
    "        elif self.has_label:\n",
    "            select_col_id = [cond_col for cond_col, *_ in query.sql.conds]\n",
    "        else:\n",
    "            select_col_id = list(range(len(query.table.header)))\n",
    "        return select_col_id\n",
    "            \n",
    "    def generate_pairs(self, query_id, query, col_id, col_name, values, op_patterns):\n",
    "        pairs = []\n",
    "        for value in values:\n",
    "            for op_pattern in op_patterns:\n",
    "                cond = op_pattern['pattern'].format(col_name=col_name, value=value)\n",
    "                cond_sql = (col_id, op_pattern['cond_op_idx'], value)\n",
    "                real_sql = {}\n",
    "                if self.has_label:\n",
    "                    real_sql = {tuple(c) for c in query.sql.conds}\n",
    "                label = 1 if cond_sql in real_sql else 0\n",
    "                pair = QuestionCondPair(query_id, query.question.text,\n",
    "                                        cond, cond_sql, label)\n",
    "                pairs.append(pair)\n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SimpleTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]')\n",
    "            else:\n",
    "                R.append('[UNK]')\n",
    "        return R\n",
    "          \n",
    "def construct_model(paths):\n",
    "    token_dict = load_vocabulary(paths.vocab)\n",
    "    tokenizer = SimpleTokenizer(token_dict)\n",
    "\n",
    "    bert_model = load_trained_model_from_checkpoint(\n",
    "        paths.config, paths.checkpoint, seq_len=None)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    x1_in = Input(shape=(None,), name='input_x1', dtype='int32')\n",
    "    x2_in = Input(shape=(None,), name='input_x2')\n",
    "    x = bert_model([x1_in, x2_in])\n",
    "    x_cls = Lambda(lambda x: x[:, 0])(x)\n",
    "    y_pred = Dense(1, activation='sigmoid', name='output_similarity')(x_cls)\n",
    "\n",
    "    model = Model([x1_in, x2_in], y_pred)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "class QuestionCondPairsDataseq(Sequence):\n",
    "    def __init__(self, dataset, tokenizer, is_train=True, max_len=120, \n",
    "                 sampler=None, shuffle=False, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "        self.sampler = sampler\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()       \n",
    "    \n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        return pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices = \\\n",
    "            self.global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "\n",
    "        X1, X2 = [], []\n",
    "        Y = []\n",
    "        \n",
    "        for data in batch_data:\n",
    "            x1, x2 = self.tokenizer.encode(first=data.question.lower(), \n",
    "                                           second=data.cond_text.lower())\n",
    "            X1.append(x1)\n",
    "            X2.append(x2)\n",
    "            if self.is_train:\n",
    "                Y.append([data.label])\n",
    "    \n",
    "        X1 = self._pad_sequences(X1, max_len=self.max_len)\n",
    "        X2 = self._pad_sequences(X2, max_len=self.max_len)\n",
    "        inputs = {'input_x1': X1, 'input_x2': X2}\n",
    "        if self.is_train:\n",
    "            Y = self._pad_sequences(Y, max_len=1)\n",
    "            outputs = {'output_similarity': Y}\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "                    \n",
    "    def on_epoch_end(self):\n",
    "        self.data = self.sampler.sample(self.dataset)\n",
    "        self.global_indices = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.global_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "def merge_result(qc_pairs, result, threshold):\n",
    "    select_result = defaultdict(set)\n",
    "    for pair, score in zip(qc_pairs, result):\n",
    "        if score > threshold:\n",
    "            select_result[pair.query_id].update([pair.cond_sql])\n",
    "    return dict(select_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NL2SQL:\n",
    "\n",
    "    def __init__(self, bert_model_path, batch_size, model_path, model2_path):\n",
    "        self.paths = get_checkpoint_paths(bert_model_path)\n",
    "        self.NUM_GPUS = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n",
    "        self.batch_size = NUM_GPUS*batch_size\n",
    "        \n",
    "        self.model = self.load_model(model_path)\n",
    "        self.model2, self.tokenizer = self.load_model2(model2_path)\n",
    "\n",
    "        self.label_encoder = SqlLabelEncoder()\n",
    "        self.token_dict = load_vocabulary(self.paths.vocab)\n",
    "        self.query_tokenizer = QueryTokenizer(self.token_dict)\n",
    "\n",
    "        self.temp_table_file = '../../TableQA-master/val/val.tables.json'\n",
    "        self.temp_data_file = {\"question\": \"你好，你知道市值超4亿，股本也超4亿股，还有收盘的价格也在4元以上的是哪些证券吗\", \"table_id\": \"69d3faa1334311e9a5bf542696d6e445\"}\n",
    "        self.temp_result = self.inference(self.temp_table_file , self.temp_data_file)\n",
    "\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        num_sel_agg = len(SQL.agg_sql_dict) + 1\n",
    "        num_cond_op = len(SQL.op_sql_dict) + 1\n",
    "        num_cond_conn_op = len(SQL.conn_sql_dict)\n",
    "\n",
    "        model = get_model(num_sel_agg,num_cond_op,num_cond_conn_op, self.paths)\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "        if self.NUM_GPUS > 1:\n",
    "            print('using {} gpus'.format(NUM_GPUS))\n",
    "            print('This is not allowed')\n",
    "        else:\n",
    "            print('using cpu or single gpu for model1')\n",
    "        return model\n",
    "\n",
    "    def load_model2(self, model_path):\n",
    "        model2, tokenizer = construct_model(self.paths)\n",
    "        model2.load_weights(model_path)\n",
    "        if self.NUM_GPUS > 1:\n",
    "            print('using {} gpus'.format(self.NUM_GPUS))\n",
    "            print('This is not allowed')\n",
    "        else:\n",
    "            print('using cpu or single gpu for model2')\n",
    "        return model2, tokenizer \n",
    "\n",
    "    def is_number(self,s):\n",
    "        try:  # 如果能运行float(s)语句，返回True（字符串s是浮点数）\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:  # ValueError为Python的一种标准异常，表示\"传入无效的参数\"\n",
    "            pass  # 如果引发了ValueError这种异常，不做任何事情（pass：不做任何事情，一般用做占位语句）\n",
    "        try:\n",
    "            import unicodedata  # 处理ASCii码的包\n",
    "            unicodedata.numeric(s)  # 把一个表示数字的字符串转换为浮点数返回的函数\n",
    "            return True\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "        return False\n",
    "        \n",
    "\n",
    "    def to_sql(self, final_pred, test_data, test_data_file):\n",
    "                \n",
    "        # agg mapping:\n",
    "        agg_mapping = {0:'',1:'AVG',2:'MAX',3:'MIN',4:'COUNT',5:'SUM'}\n",
    "        # cond symbol mapping:\n",
    "        cond_op_mapping = {0:'>',1:'<',2:'=',3:'<>'}\n",
    "        # cond connect mapping\n",
    "        cond_conn_mapping = {0:'', 1:'and', 2:'or'}\n",
    "\n",
    "        sql = 'SELECT '\n",
    "\n",
    "        agg_index_list = final_pred['agg']\n",
    "        sel_index_list = final_pred['sel']\n",
    "\n",
    "        for i, agg_index in enumerate(agg_index_list):\n",
    "            if agg_index == 0:\n",
    "                sql+=test_data[0].table.header[sel_index_list[i]][0]\n",
    "            else:\n",
    "                sql+=agg_mapping[agg_index]\n",
    "                sql+='('\n",
    "                sql+=test_data[0].table.header[sel_index_list[i]][0]\n",
    "                sql+=')'\n",
    "            if i!=len(agg_index_list)-1:\n",
    "                sql+=','\n",
    "\n",
    "        sql += ' FROM ' + test_data_file['table_id']\n",
    "\n",
    "        if len(final_pred['conds'])>=1:\n",
    "            sql += ' WHERE '\n",
    "            for i, (cond_col, cond_op, cond_value) in enumerate(final_pred['conds']):\n",
    "                sql += test_data[0].table.header[cond_col][0]\n",
    "                sql += cond_op_mapping[cond_op]\n",
    "                if not self.is_number(cond_value):\n",
    "                    sql += '\\'' + cond_value + '\\'' \n",
    "                else:\n",
    "                    sql += cond_value\n",
    "                if i!=len(final_pred['conds'])-1:\n",
    "                    sql += ' '+cond_conn_mapping[final_pred['cond_conn_op']]+' '\n",
    "\n",
    "        return sql\n",
    "\n",
    "    def merge_result(self, qc_pairs, result, threshold=0.01):\n",
    "    # merge result depending on manually set threshold\n",
    "        select_result = defaultdict(set)\n",
    "        for pair, score in zip(qc_pairs, result):\n",
    "            if score > threshold:\n",
    "                select_result[pair.query_id].update([pair.cond_sql])\n",
    "        return dict(select_result)\n",
    "\n",
    "    def merge_result2(self, qc_pairs, task1_result, task2_result):\n",
    "    # merge result depending on automatically set result from model1's prediction\n",
    "        if len(task1_result[0]['conds'])>0:\n",
    "            threshold = np.sort(np.squeeze(task2_result))[::-1][:len(task1_result[0]['conds'])][-1] - 1e-7\n",
    "        else:\n",
    "            threshold = 0.01\n",
    "        select_result = defaultdict(set)\n",
    "        for pair, score in zip(qc_pairs, task2_result):\n",
    "            if score > threshold:\n",
    "                select_result[pair.query_id].update([pair.cond_sql])\n",
    "        return dict(select_result)\n",
    "\n",
    "    def inference(self, table_file, data, threshold=0.8, detail=False):\n",
    "        test_tables = read_tables(table_file)\n",
    "        test_data = read_single_data(data, test_tables)\n",
    "\n",
    "\n",
    "        test_dataseq = DataSequence(\n",
    "            data=test_data, \n",
    "            tokenizer=self.query_tokenizer,\n",
    "            label_encoder=self.label_encoder,\n",
    "            is_train=False, \n",
    "            shuffle_header=False,\n",
    "            max_len=160, \n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "        pred_sqls = []\n",
    "\n",
    "        for batch_data in test_dataseq:\n",
    "            header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n",
    "            preds_cond_conn_op, preds_sel_agg, preds_cond_op = self.model.predict_on_batch(batch_data)\n",
    "            sqls = outputs_to_sqls(\n",
    "                preds_cond_conn_op, \n",
    "                preds_sel_agg, \n",
    "                preds_cond_op, \n",
    "                header_lens, \n",
    "                test_dataseq.label_encoder\n",
    "            )\n",
    "            pred_sqls += sqls\n",
    "\n",
    "        task1_result = copy.deepcopy(pred_sqls)  # deep copy then return task1_result for debugging\n",
    "        \n",
    "        test_qc_pairs = QuestionCondPairsDataset(\n",
    "            test_data, \n",
    "            candidate_extractor=CandidateCondsExtractor(share_candidates=True),\n",
    "            has_label=False,\n",
    "            model_1_outputs=pred_sqls\n",
    "        )\n",
    "\n",
    "        test_qc_pairs_seq = QuestionCondPairsDataseq(\n",
    "            test_qc_pairs, \n",
    "            self.tokenizer,                 \n",
    "            sampler=FullSampler(), \n",
    "            shuffle=False, \n",
    "            batch_size=128\n",
    "        )\n",
    "\n",
    "        task2_result = self.model2.predict_generator(test_qc_pairs_seq, verbose=1)\n",
    "\n",
    "        # task2_result_merged = self.merge_result(test_qc_pairs, task2_result, threshold=threshold)  \n",
    "        task2_result_merged = self.merge_result2(test_qc_pairs, task1_result, task2_result)  \n",
    "\n",
    "\n",
    "        final_result = None\n",
    "        for query_id, pred_sql in enumerate(pred_sqls):\n",
    "            cond = list(task2_result_merged.get(query_id, []))\n",
    "            pred_sql['conds'] = cond\n",
    "            final_result = pred_sql\n",
    "        \n",
    "        sql = self.to_sql(final_result, test_data, data)\n",
    "        \n",
    "        if detail:\n",
    "            return sql, task1_result[0], task2_result, task2_result_merged, final_result, test_data, test_qc_pairs_seq\n",
    "        else: \n",
    "            return sql, test_tables[data['table_id']]._df   # sql & table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "\n",
    "bert_model_path = '../../../NLP/experiments/model/chinese_wwm_L-12_H-768_A-12'\n",
    "\n",
    "batch_size = 32\n",
    "model_path = '../model/m1_ep30.h5'\n",
    "model2_path = '../model/m2.h5'  \n",
    "\n",
    "nl2sql = NL2SQL(bert_model_path, batch_size, model_path, model2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building candidate cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengyuan/anaconda3/envs/nl2sql/lib/python3.6/site-packages/ipykernel_launcher.py:102: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7be65911d5a4bbeb31d7d42ab005512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 803ms/step\n",
      "SELECT 证券简称 FROM 69d3faa1334311e9a5bf542696d6e445 WHERE 收盘价(元)>4 and 总市值(亿元)>4 and 总股本(亿股)>4\n"
     ]
    }
   ],
   "source": [
    "# Inference output test\n",
    "\n",
    "test_table_file = '../../TableQA-master/val/val.tables.json'\n",
    "test_data_file = {\"question\": \"你好，你知道市值超4亿，股本也超4亿股，还有收盘的价格也在4元以上的是哪些证券吗\", \"table_id\": \"69d3faa1334311e9a5bf542696d6e445\"}\n",
    "\n",
    "sql, table =  nl2sql.inference(test_table_file, test_data_file, detail = False)\n",
    "\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>证券简称</th>\n",
       "      <th>收盘价(元)</th>\n",
       "      <th>总股本(亿股)</th>\n",
       "      <th>总市值(亿元)</th>\n",
       "      <th>每股预收(元)</th>\n",
       "      <th>市净率</th>\n",
       "      <th>EPS2011</th>\n",
       "      <th>EPS2012E</th>\n",
       "      <th>EPS2013E</th>\n",
       "      <th>PE2011</th>\n",
       "      <th>PE2012E</th>\n",
       "      <th>PE2013E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>万科A</td>\n",
       "      <td>8.98</td>\n",
       "      <td>109.95</td>\n",
       "      <td>987.37</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.52</td>\n",
       "      <td>10.2</td>\n",
       "      <td>7.66</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>保利地产</td>\n",
       "      <td>12.51</td>\n",
       "      <td>59.48</td>\n",
       "      <td>744.14</td>\n",
       "      <td>8.67</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.85</td>\n",
       "      <td>11.37</td>\n",
       "      <td>8.73</td>\n",
       "      <td>6.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>招商地产</td>\n",
       "      <td>22.82</td>\n",
       "      <td>17.17</td>\n",
       "      <td>391.89</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.51</td>\n",
       "      <td>15.12</td>\n",
       "      <td>11.82</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>金地集团</td>\n",
       "      <td>6.48</td>\n",
       "      <td>44.72</td>\n",
       "      <td>289.75</td>\n",
       "      <td>4.22</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.67</td>\n",
       "      <td>7.78</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>金融街</td>\n",
       "      <td>6.73</td>\n",
       "      <td>30.27</td>\n",
       "      <td>203.72</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.11</td>\n",
       "      <td>10.04</td>\n",
       "      <td>7.98</td>\n",
       "      <td>6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>嘉凯城</td>\n",
       "      <td>4.87</td>\n",
       "      <td>18.04</td>\n",
       "      <td>87.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>44.27</td>\n",
       "      <td>20.99</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>华侨城A</td>\n",
       "      <td>7.59</td>\n",
       "      <td>55.93</td>\n",
       "      <td>424.54</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.83</td>\n",
       "      <td>13.36</td>\n",
       "      <td>10.69</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<nl2sql.utils.Table at 0x7f894059fcf8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building candidate cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengyuan/anaconda3/envs/nl2sql/lib/python3.6/site-packages/ipykernel_launcher.py:102: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de5f6c55e4459c8f1106c85fb56844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 297ms/step\n",
      "SELECT 影片 FROM 4d280ca33aaa11e9b11af40f24344a08 WHERE 类型='恐怖' and 北美映期='2019-01-04'\n"
     ]
    }
   ],
   "source": [
    "test_data_file = {\"question\": \"麻烦告诉我，有哪些影片是恐怖类型的并且北美上映日期是2019年1月4日的？\", \"table_id\": \"4d280ca33aaa11e9b11af40f24344a08\"}\n",
    "\n",
    "sql, table =  nl2sql.inference(test_table_file, test_data_file, detail = False)\n",
    "\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>北美映期</th>\n",
       "      <th>影片</th>\n",
       "      <th>导演</th>\n",
       "      <th>主演</th>\n",
       "      <th>类型</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>伊莱</td>\n",
       "      <td>夏兰·福伊</td>\n",
       "      <td>凯利・莱利，莉丽・泰勒，马克斯马丁尼</td>\n",
       "      <td>恐怖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>密室逃生</td>\n",
       "      <td>亚当·罗比塔</td>\n",
       "      <td>黛博拉·安·沃尔，泰勒·莱伯恩，泰勒·拉塞尔</td>\n",
       "      <td>恐怖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>一条狗的回家路</td>\n",
       "      <td>查尔斯·马丁·史密斯</td>\n",
       "      <td>艾什莉·贾德、爱德华·詹姆斯·奥莫斯</td>\n",
       "      <td>剧情/喜剧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>美版触不可及</td>\n",
       "      <td>尼尔·博格</td>\n",
       "      <td>布莱恩·科兰斯顿、凯文·哈特</td>\n",
       "      <td>剧情/喜剧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>玻璃先生</td>\n",
       "      <td>奈特·沙马兰</td>\n",
       "      <td>詹姆斯·麦卡沃伊、布鲁斯·威利斯</td>\n",
       "      <td>悬疑/惊悚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>王者少年</td>\n",
       "      <td>乔·考尼什</td>\n",
       "      <td>路易斯·阿什伯恩·瑟金斯、帕特里克·斯图尔特</td>\n",
       "      <td>奇幻</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<nl2sql.utils.Table at 0x7f89403ae978>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building candidate cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengyuan/anaconda3/envs/nl2sql/lib/python3.6/site-packages/ipykernel_launcher.py:102: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4a2f1b15a949609576e276b19f07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 685ms/step\n",
      "SELECT 院线,目前影院数量 FROM 4d278c853aaa11e9aeb5f40f24344a08 WHERE 2018年票房（亿元）>15\n"
     ]
    }
   ],
   "source": [
    "test_data_file = {\"question\": \"你能就是帮我查一下18年票房高于15亿的电影院线现在影院数是多少啊，都是什么院线啊\", \"table_id\": \"4d278c853aaa11e9aeb5f40f24344a08\"}\n",
    "\n",
    "sql, table =  nl2sql.inference(test_table_file, test_data_file, detail = False)\n",
    "\n",
    "print(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>院线</th>\n",
       "      <th>2018年票房（亿元）</th>\n",
       "      <th>目前影院数量</th>\n",
       "      <th>2018年底银幕数量</th>\n",
       "      <th>单银幕票房（万元）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>横店影视股份有限公司</td>\n",
       "      <td>24.6</td>\n",
       "      <td>415.0</td>\n",
       "      <td>2517.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>北京华夏联合电影院线</td>\n",
       "      <td>20.43</td>\n",
       "      <td>415.0</td>\n",
       "      <td>2333.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>江苏幸福蓝海院线有限责任公司</td>\n",
       "      <td>19.52</td>\n",
       "      <td>347.0</td>\n",
       "      <td>2095.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>浙江时代电影院线股份有限公司</td>\n",
       "      <td>17.17</td>\n",
       "      <td>351.0</td>\n",
       "      <td>2225.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>四川太平洋电影院线有限责任公司</td>\n",
       "      <td>16.36</td>\n",
       "      <td>336.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>重庆保利万和电影院线</td>\n",
       "      <td>13.51</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>河南奥斯卡院线有限责任公司</td>\n",
       "      <td>11.07</td>\n",
       "      <td>279.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<nl2sql.utils.Table at 0x7f8940734470>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pred: SQL in dict\n",
    "# final_pred = {'sel': [0,1], 'agg': [1,2], 'cond_conn_op': 1, 'conds': [(1, 0, '1000'), (2, 2, '上海')]}\n",
    "# # input: question\n",
    "# test_data_file['question']\n",
    "# # input: table id \n",
    "# test_data_file['table_id']\n",
    "# # input: table \n",
    "# test_data[0].table\n",
    "# # input: table header\n",
    "# test_data[0].table.header\n",
    "# # agg mapping:\n",
    "# agg_mapping = {0:'',1:'AVG',2:'MAX',3:'MIN',4:'COUNT',5:'SUM'}\n",
    "# # cond symbol mapping:\n",
    "# cond_op_mapping = {0:'>',1:'<',2:'=',3:'<>'}\n",
    "# # cond connect mapping\n",
    "# cond_conn_mapping = {0:'', 1:'and', 2:'or'}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8317ad8e1b94d3e5967c07b4286028fbd85085f8c3f6f67fe8bca444bf75006d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nl2sql': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
