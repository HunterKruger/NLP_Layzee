{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import cn2an\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nl2sql.utils import read_data, read_tables, SQL, Query, Question, Table\n",
    "from keras_bert import get_checkpoint_paths, load_vocabulary, Tokenizer, load_trained_model_from_checkpoint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import multi_gpu_model\n",
    "import keras\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''   \n",
    "NUM_GPUS = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]')\n",
    "            else:\n",
    "                R.append('[UNK]')\n",
    "        return R\n",
    "        \n",
    "        \n",
    "def construct_model(paths):\n",
    "    token_dict = load_vocabulary(paths.vocab)\n",
    "    tokenizer = SimpleTokenizer(token_dict)\n",
    "\n",
    "    bert_model = load_trained_model_from_checkpoint(\n",
    "        paths.config, paths.checkpoint, seq_len=None)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    x1_in = Input(shape=(None,), name='input_x1', dtype='int32')\n",
    "    x2_in = Input(shape=(None,), name='input_x2')\n",
    "    x = bert_model([x1_in, x2_in])\n",
    "    x_cls = Lambda(lambda x: x[:, 0])(x)\n",
    "    y_pred = Dense(1, activation='sigmoid', name='output_similarity')(x_cls)\n",
    "\n",
    "    model = Model([x1_in, x2_in], y_pred)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_path = '../../../experiments/model/chinese_wwm_L-12_H-768_A-12'\n",
    "paths = get_checkpoint_paths(bert_model_path)\n",
    "\n",
    "model, tokenizer = construct_model(paths)\n",
    "\n",
    "model_path = '../model/m2.h5'  \n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../model/m2/0/saved_model.pb\n",
      "save model pb success ...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.util import compat\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def export_savedmodel(model,output_path):\n",
    "    '''\n",
    "    传入keras model会自动保存为pb格式\n",
    "    '''\n",
    "    model_path = output_path # 模型保存的路径\n",
    "    model_version = 0 # 模型保存的版本\n",
    "    # 从网络的输入输出创建预测的签名\n",
    "\n",
    "    input_dict = dict()\n",
    "    for input_ in model.input:\n",
    "        input_dict[input_.name]=input_\n",
    "\n",
    "    \n",
    "    model_signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "        inputs=input_dict, outputs={'output':model.output}\n",
    "    )\n",
    "    # 使用utf-8编码将 字节或Unicode 转换为字节\n",
    "    export_path = os.path.join(compat.as_bytes(model_path), compat.as_bytes(str(model_version))) # 将保存路径和版本号join\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(export_path) # 生成\"savedmodel\"协议缓冲区并保存变量和模型\n",
    "    builder.add_meta_graph_and_variables( # 将当前元图添加到savedmodel并保存变量\n",
    "    sess=K.get_session(), # 返回一个 session 默认返回tf的sess,否则返回keras的sess,两者都没有将创建一个全新的sess返回\n",
    "    tags=[tf.saved_model.tag_constants.SERVING], # 导出模型tag为SERVING(其他可选TRAINING,EVAL,GPU,TPU)\n",
    "    clear_devices=True, # 清除设备信息\n",
    "    signature_def_map={ # 签名定义映射\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: # 默认服务签名定义密钥\n",
    "        model_signature # 网络的输入输出策创建预测的签名\n",
    "    })\n",
    "    builder.save() # 将\"savedmodel\"协议缓冲区写入磁盘.\n",
    "    print(\"save model pb success ...\")\n",
    "\n",
    "output_path = '../model/m2'\n",
    "export_savedmodel(model, output_path) # 将模型传入保存模型的方法内,模型保存成功."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8317ad8e1b94d3e5967c07b4286028fbd85085f8c3f6f67fe8bca444bf75006d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nl2sql': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
