{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class Header:\n",
    "    def __init__(self, names: list, types: list):\n",
    "        self.names = names\n",
    "        self.types = types\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.names[idx], self.types[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ' | '.join(['{}({})'.format(n, t) for n, t in zip(self.names, self.types)])\n",
    "\n",
    "class Table:\n",
    "    def __init__(self, id, name, title, header: Header, rows, **kwargs):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.title = title\n",
    "        self.header = header\n",
    "        self.rows = rows\n",
    "        self._df = None\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        if self._df is None:\n",
    "            self._df = pd.DataFrame(data=self.rows,\n",
    "                                    columns=self.header.names,\n",
    "                                    dtype=str)\n",
    "        return self._df\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.df._repr_html_()\n",
    "\n",
    "class Tables:\n",
    "    table_dict = None\n",
    "\n",
    "    def __init__(self, table_list: list = None, table_dict: dict = None):\n",
    "        self.table_dict = {}\n",
    "        if isinstance(table_list, list):\n",
    "            for table in table_list:\n",
    "                self.table_dict[table.id] = table\n",
    "        if isinstance(table_dict, dict):\n",
    "            self.table_dict.update(table_dict)\n",
    "\n",
    "    def push(self, table):\n",
    "        self.table_dict[table.id] = table\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table_dict)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Tables(\n",
    "            table_list=list(self.table_dict.values()) +\n",
    "            list(other.table_dict.values())\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        return self.table_dict[id]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for table_id, table in self.table_dict.items():\n",
    "            yield table_id, table\n",
    "\n",
    "class Question:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "class SQL:\n",
    "    op_sql_dict = {0: \">\", 1: \"<\", 2: \"==\", 3: \"!=\"}\n",
    "    agg_sql_dict = {0: \"\", 1: \"AVG\", 2: \"MAX\", 3: \"MIN\", 4: \"COUNT\", 5: \"SUM\"}\n",
    "    conn_sql_dict = {0: \"NULL\", 1: \"AND\", 2: \"OR\"}\n",
    "\n",
    "    def __init__(self, cond_conn_op: int, agg: list, sel: list, conds: list, **kwargs):\n",
    "        self.cond_conn_op = cond_conn_op\n",
    "        self.sel = []\n",
    "        self.agg = []\n",
    "        sel_agg_pairs = zip(sel, agg)\n",
    "        sel_agg_pairs = sorted(sel_agg_pairs, key=lambda x: x[0])\n",
    "        for col_id, agg_op in sel_agg_pairs:\n",
    "            self.sel.append(col_id)\n",
    "            self.agg.append(agg_op)\n",
    "        self.conds = sorted(conds, key=lambda x: x[0])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        return cls(**data)\n",
    "\n",
    "    def keys(self):\n",
    "        return ['cond_conn_op', 'sel', 'agg', 'conds']\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(dict(self), ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "    def equal_all_mode(self, other):\n",
    "        return self.to_json() == other.to_json()\n",
    "\n",
    "    def equal_agg_mode(self, other):\n",
    "        self_sql = SQL(cond_conn_op=0, agg=self.agg, sel=self.sel, conds=[])\n",
    "        other_sql = SQL(cond_conn_op=0, agg=other.agg, sel=other.sel, conds=[])\n",
    "        return self_sql.to_json() == other_sql.to_json()\n",
    "\n",
    "    def equal_conn_and_agg_mode(self, other):\n",
    "        self_sql = SQL(cond_conn_op=self.cond_conn_op,\n",
    "                       agg=self.agg,\n",
    "                       sel=self.sel,\n",
    "                       conds=[])\n",
    "        other_sql = SQL(cond_conn_op=other.cond_conn_op,\n",
    "                        agg=other.agg,\n",
    "                        sel=other.sel,\n",
    "                        conds=[])\n",
    "        return self_sql.to_json() == other_sql.to_json()\n",
    "\n",
    "    def equal_no_val_mode(self, other):\n",
    "        self_sql = SQL(cond_conn_op=self.cond_conn_op,\n",
    "                       agg=self.agg,\n",
    "                       sel=self.sel,\n",
    "                       conds=[cond[:2] for cond in self.conds])\n",
    "        other_sql = SQL(cond_conn_op=other.cond_conn_op,\n",
    "                        agg=other.agg,\n",
    "                        sel=other.sel,\n",
    "                        conds=[cond[:2] for cond in other.conds])\n",
    "        return self_sql.to_json() == other_sql.to_json()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        raise NotImplementedError('compare mode not set')\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = ''\n",
    "        repr_str += \"sel: {}\\n\".format(self.sel)\n",
    "        repr_str += \"agg: {}\\n\".format([self.agg_sql_dict[a]\n",
    "                                        for a in self.agg])\n",
    "        repr_str += \"cond_conn_op: '{}'\\n\".format(\n",
    "            self.conn_sql_dict[self.cond_conn_op])\n",
    "        repr_str += \"conds: {}\".format(\n",
    "            [[cond[0], self.op_sql_dict[cond[1]], cond[2]] for cond in self.conds])\n",
    "\n",
    "        return repr_str\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.__repr__().replace('\\n', '<br>')\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, question: Question, table: Table, sql: SQL = None):\n",
    "        self.question = question\n",
    "        self.table = table\n",
    "        self.sql = sql\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        repr_str = '{}<br>{}<br>{}'.format(\n",
    "            self.table._repr_html_(),\n",
    "            self.question.__repr__(),\n",
    "            self.sql._repr_html_() if self.sql is not None else ''\n",
    "        )\n",
    "        return repr_str\n",
    "\n",
    "def read_tables(table_file):\n",
    "    tables = Tables()\n",
    "    with open(table_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tb = json.loads(line)\n",
    "            header = Header(tb.pop('header'), tb.pop('types'))\n",
    "            table = Table(header=header, **tb)\n",
    "            tables.push(table)\n",
    "    return tables\n",
    "\n",
    "def read_data(data_file, tables: Tables):\n",
    "    queries = []\n",
    "    with open(data_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            question = Question(text=data['question'])\n",
    "            table = tables[data['table_id']]\n",
    "            if 'sql' in data:\n",
    "                sql = SQL.from_dict(data['sql'])\n",
    "            else:\n",
    "                sql = None\n",
    "            query = Query(question=question, table=table, sql=sql)\n",
    "            queries.append(query)\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_file = '../../TableQA-master/train/train.tables.json'\n",
    "train_data_file = '../../TableQA-master/train/train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables = read_tables(train_table_file)\n",
    "train_data = read_data(train_data_file, train_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_file = '../../TableQA-master/test/test.tables.json'\n",
    "test_data_file = '../../TableQA-master/test/test.json'\n",
    "\n",
    "test_tables = read_tables(test_table_file)\n",
    "test_data = read_data(test_data_file, test_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].sql is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>影片名称</th>\n",
       "      <th>周票房（万）</th>\n",
       "      <th>票房占比（%）</th>\n",
       "      <th>场均人次</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>死侍2：我爱我家</td>\n",
       "      <td>10637.3</td>\n",
       "      <td>25.8</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>白蛇：缘起</td>\n",
       "      <td>10503.8</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大黄蜂</td>\n",
       "      <td>6426.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>密室逃生</td>\n",
       "      <td>5841.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“大”人物</td>\n",
       "      <td>3322.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>家和万事惊</td>\n",
       "      <td>635.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>钢铁飞龙之奥特曼崛起</td>\n",
       "      <td>595.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>海王</td>\n",
       "      <td>500.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>一条狗的回家路</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>掠食城市</td>\n",
       "      <td>356.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br>二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀<br>sel: [2]<br>agg: ['SUM']<br>cond_conn_op: 'OR'<br>conds: [[0, '==', '大黄蜂'], [0, '==', '密室逃生']]"
      ],
      "text/plain": [
       "<__main__.Query at 0x7f35bc535208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(train_data[0].question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "sel: [4]<br>agg: ['SUM']<br>cond_conn_op: 'NULL'<br>conds: [[2, '==', '湖南卫视']]"
      ],
      "text/plain": [
       "sel: [4]\n",
       "agg: ['SUM']\n",
       "cond_conn_op: 'NULL'\n",
       "conds: [[2, '==', '湖南卫视']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 333\n",
    "train_data[idx].sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NULL'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[idx].sql.conn_sql_dict[train_data[idx].sql.cond_conn_op]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NULL-1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[idx].sql.conn_sql_dict[train_data[idx].sql.cond_conn_op]+'-'+str(len(train_data[idx].sql.conds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[idx].sql.conds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>时间</th>\n",
       "      <th>锂电池需求量</th>\n",
       "      <th>锂电池YoY</th>\n",
       "      <th>三元电池需求量</th>\n",
       "      <th>三元电池YoY</th>\n",
       "      <th>磷酸铁锂&amp;钴酸锂需求量</th>\n",
       "      <th>磷酸铁锂&amp;钴酸锂YoY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016A</td>\n",
       "      <td>28.3</td>\n",
       "      <td>--</td>\n",
       "      <td>7.5</td>\n",
       "      <td>--</td>\n",
       "      <td>20.8</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017A</td>\n",
       "      <td>36.2</td>\n",
       "      <td>27.8%</td>\n",
       "      <td>15.8</td>\n",
       "      <td>111.9%</td>\n",
       "      <td>20.4</td>\n",
       "      <td>-2.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018E</td>\n",
       "      <td>48.3</td>\n",
       "      <td>33.6%</td>\n",
       "      <td>27.5</td>\n",
       "      <td>74.2%</td>\n",
       "      <td>20.8</td>\n",
       "      <td>2.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019E</td>\n",
       "      <td>73.8</td>\n",
       "      <td>52.7%</td>\n",
       "      <td>52.2</td>\n",
       "      <td>89.5%</td>\n",
       "      <td>21.6</td>\n",
       "      <td>4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020E</td>\n",
       "      <td>110.0</td>\n",
       "      <td>49.1%</td>\n",
       "      <td>88.6</td>\n",
       "      <td>69.9%</td>\n",
       "      <td>21.4</td>\n",
       "      <td>-1.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<__main__.Table at 0x7f35c4dd1ac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[222].table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "影片名称(text) | 周票房（万）(real) | 票房占比（%）(real) | 场均人次(real)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].table.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('影片名称', 'text')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].table.header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41522"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'影片名称|周票房（万）|票房占比（%）|场均人次'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_sent = ''\n",
    "for header, type in train_data[0].table.header:\n",
    "    header_sent+=header\n",
    "    header_sent+='|'\n",
    "header_sent = header_sent[:-1]\n",
    "header_sent  # use it as sent2 in BertTokenizer, then modifiy the '|' id in token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 11, 19]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_list = [i for i, value in enumerate(header_sent) if value == '|']\n",
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import config\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    # customized dataset\n",
    "    # implement ___len___ & __getitem__ function\n",
    "\n",
    "    def __init__(self, data, has_label=True, max_len=config.MAX_LEN, model_name=config.BASE_MODEL_PATH, SEP_temp='|', cls_token='[unused8]'):\n",
    "        self.data = data                                                                     # loaded data\n",
    "        self.max_len = max_len                                                               # max length of sequence\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cls_token=cls_token)      # cls_token will be replaced by a unused token in bert vocab\n",
    "        self.indexes = np.arange(len(self.data))                                             # set a list of indexes according to data length\n",
    "        self.has_label = has_label                                                           # bool, input data contains labels or not\n",
    "        self.SEP_temp = SEP_temp                                                             # temporarily seperate headers, len(SEP_temp) should be 1, then will be replaced by [SEP]'s token id \n",
    "        self.SEP_temp_id = self.tokenizer.encode(SEP_temp)[1]                                # SEP_temp's token id \n",
    "        self.SEP_id = self.tokenizer.encode('[SEP]')[1]                                      # SEP's token id \n",
    "        self.XLS_id = self.tokenizer.encode(cls_token)[1]                                    # XLS_temp's token id \n",
    "\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        question = str(self.data[idx].question)\n",
    "\n",
    "        # construct header in str\n",
    "        header_sent = ''\n",
    "        for header, _ in self.data[idx].table.header:\n",
    "            header_sent+=header\n",
    "            header_sent+=self.SEP_temp\n",
    "        header_sent = header_sent[:-1]                # drop the SEP_temp at tail\n",
    "\n",
    "        # print(question)\n",
    "        # print(header_sent)\n",
    "\n",
    "        embeddings = self.tokenizer(\n",
    "            question, header_sent,                    # sentence 1, sentence 2\n",
    "            padding='max_length',                     # Pad to max_length\n",
    "            truncation=True,                          # Truncate to max_length\n",
    "            max_length=self.max_len,                  # Set max_length\n",
    "            return_tensors='pt'                       # Return torch.Tensor objects\n",
    "        )\n",
    "\n",
    "        token_ids = torch.squeeze(embeddings['input_ids'])                                             # tensor of token ids\n",
    "        token_ids = torch.where(token_ids==self.SEP_temp_id, torch.tensor(self.SEP_id), token_ids)     # replace SEP_temp_id by SEP_id\n",
    "\n",
    "        attention_masks = torch.squeeze(embeddings['attention_mask'])    # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = torch.squeeze(embeddings['token_type_ids'])     # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        SEP_position_list = [i for i, value in enumerate(token_ids) if value == self.SEP_id]\n",
    "\n",
    "\n",
    "        # True if the dataset has labels (when training or validating or testing)\n",
    "        if self.has_label:                                               \n",
    "            W_num_op_label = self.data[idx].sql.conn_sql_dict[self.data[idx].sql.cond_conn_op] + '-' + str(len(self.data[idx].sql.conds)) \n",
    "            S_num_label = len(self.data[idx].sql.sel)\n",
    "\n",
    "            return {\n",
    "                ## X\n",
    "                'token_ids': token_ids,\n",
    "                'token_type_ids': token_type_ids,\n",
    "                'attention_masks': attention_masks,\n",
    "                'SEP_position_list': torch.tensor(SEP_position_list),\n",
    "\n",
    "                ## y\n",
    "                'S_num': torch.tensor(config.S_num_label2id[S_num_label]),              \n",
    "                'W_num_op': torch.tensor(config.W_num_op_label2id[W_num_op_label])\n",
    "            } \n",
    "        # False if the dataset do not have labels (when inferencing)\n",
    "        else:                                                           \n",
    "            return {\n",
    "                'token_ids': token_ids,\n",
    "                'token_type_ids': token_type_ids,\n",
    "                'attention_masks': attention_masks,\n",
    "                'SEP_position_list': torch.tensor(SEP_position_list)\n",
    "            }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tables = read_tables(config.val_table_file)\n",
    "val_data = read_data(config.val_data_file, val_tables)\n",
    "val_set = CustomDataset(val_data)\n",
    "\n",
    "\n",
    "test_tables = read_tables(config.test_table_file)\n",
    "test_data = read_data(config.test_data_file, test_tables)\n",
    "test_set = CustomDataset(test_data,has_label=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import config\n",
    "\n",
    "class CustomDataset2(Dataset):\n",
    "\n",
    "    # customized dataset\n",
    "    # implement ___len___ & __getitem__ function\n",
    "\n",
    "    def __init__(self, data, has_label=True, max_len=config.MAX_LEN, model_name=config.BASE_MODEL_PATH, SEP_temp='|', cls_token='[unused8]'):\n",
    "        self.data = data                                                                     # loaded data\n",
    "        self.max_len = max_len                                                               # max length of sequence\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cls_token=cls_token)      # cls_token will be replaced by a unused token in bert vocab\n",
    "        self.indexes = np.arange(len(self.data))                                             # set a list of indexes according to data length\n",
    "        self.has_label = has_label                                                           # bool, input data contains labels or not\n",
    "        self.SEP_temp = SEP_temp                                                             # temporarily seperate headers, len(SEP_temp) should be 1, then will be replaced by [SEP]'s token id \n",
    "        self.SEP_temp_id = self.tokenizer.encode(SEP_temp)[1]                                # SEP_temp's token id \n",
    "        self.SEP_id = self.tokenizer.encode('[SEP]')[1]                                      # SEP's token id \n",
    "        self.XLS_id = self.tokenizer.encode(cls_token)[1]                                    # XLS_temp's token id \n",
    "\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        question = str(self.data[idx].question)\n",
    "\n",
    "        # construct header in str\n",
    "        header_sent = ''\n",
    "        for header, _ in self.data[idx].table.header:\n",
    "            header_sent+=header\n",
    "            header_sent+=self.SEP_temp\n",
    "        header_sent = header_sent[:-1]                # drop the SEP_temp at tail\n",
    "\n",
    "        # print(question)\n",
    "        # print(header_sent)\n",
    "\n",
    "        embeddings = self.tokenizer(\n",
    "            question, header_sent,                    # sentence 1, sentence 2\n",
    "            padding='max_length',                     # Pad to max_length\n",
    "            truncation=True,                          # Truncate to max_length\n",
    "            max_length=self.max_len,                  # Set max_length\n",
    "            return_tensors='pt'                       # Return torch.Tensor objects\n",
    "        )\n",
    "\n",
    "        token_ids = torch.squeeze(embeddings['input_ids'])                                             # tensor of token ids\n",
    "        token_ids = torch.where(token_ids==self.SEP_temp_id, torch.tensor(self.SEP_id), token_ids)     # replace SEP_temp_id by SEP_id\n",
    "\n",
    "        attention_masks = torch.squeeze(embeddings['attention_mask'])    # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = torch.squeeze(embeddings['token_type_ids'])     # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "\n",
    "        # True if the dataset has labels (when training or validating or testing)\n",
    "        if self.has_label:                                               \n",
    "            W_num_op_label = self.data[idx].sql.conn_sql_dict[self.data[idx].sql.cond_conn_op] + '-' + str(len(self.data[idx].sql.conds)) \n",
    "            S_num_label = len(self.data[idx].sql.sel)\n",
    "\n",
    "            return {\n",
    "                ## X\n",
    "                'token_ids': token_ids,\n",
    "                'token_type_ids': token_type_ids,\n",
    "                'attention_masks': attention_masks,\n",
    "\n",
    "                ## y\n",
    "                'S_num': torch.tensor(config.S_num_label2id[S_num_label]),              \n",
    "                'W_num_op': torch.tensor(config.W_num_op_label2id[W_num_op_label])\n",
    "            } \n",
    "        # False if the dataset do not have labels (when inferencing)\n",
    "        else:                                                           \n",
    "            return {\n",
    "                'token_ids': token_ids,\n",
    "                'token_type_ids': token_type_ids,\n",
    "                'attention_masks': attention_masks,\n",
    "            }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,13,4,3]\n",
    "a[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_data):\n",
    "\n",
    "    batch_data.sort(key=lambda xi: len(xi['SEP_position_list']), reverse=True)\n",
    "    \n",
    "    SEP_position_list_seq = [xi['SEP_position_list'] for xi in batch_data]\n",
    "\n",
    "    token_ids_seq = [xi['token_ids'] for xi in batch_data]\n",
    "    token_type_ids_seq = [xi['token_type_ids'] for xi in batch_data]\n",
    "    attention_masks_seq = [xi['attention_masks'] for xi in batch_data]\n",
    "    padded_SEP_position_list_seq = torch.nn.utils.rnn.pad_sequence(SEP_position_list_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    S_num_seq = [xi['S_num'] for xi in batch_data]\n",
    "    W_num_op_seq = [xi['W_num_op'] for xi in batch_data]\n",
    "\n",
    "    return {\n",
    "        'token_ids': token_ids_seq, \n",
    "        'token_type_ids': token_type_ids_seq,\n",
    "        'attention_masks': attention_masks_seq,\n",
    "        'SEP_position_list': padded_SEP_position_list_seq,\n",
    "        'S_num': S_num_seq,\n",
    "        'W_num_op': W_num_op_seq\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "def collate_fn_labelless(batch_data):\n",
    "\n",
    "    batch_data.sort(key=lambda xi: len(xi['SEP_position_list']), reverse=True)\n",
    "    \n",
    "    SEP_position_list_seq = [xi['SEP_position_list'] for xi in batch_data]\n",
    "\n",
    "    token_ids_seq = [xi['token_ids'] for xi in batch_data]\n",
    "    token_type_ids_seq = [xi['token_type_ids'] for xi in batch_data]\n",
    "    attention_masks_seq = [xi['attention_masks'] for xi in batch_data]\n",
    "    padded_SEP_position_list_seq = torch.nn.utils.rnn.pad_sequence(SEP_position_list_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'token_ids': token_ids_seq, \n",
    "        'token_type_ids': token_type_ids_seq,\n",
    "        'attention_masks': attention_masks_seq,\n",
    "        'SEP_position_list': padded_SEP_position_list_seq\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_set2 = CustomDataset2(val_data)\n",
    "val_dataloader2 = DataLoader(\n",
    "    dataset=val_set2, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_set, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_set, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn_labelless\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0), tensor(0)]\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "for data in val_dataloader:\n",
    "    print(data['S_num'])\n",
    "    print(torch.stack(data['S_num']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "for data in val_dataloader2:\n",
    "    print(data['S_num'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34, 37, 44, 50, 58, 65, 77, 88])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[idx]['SEP_position_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 44, 50, 58, 65, 77, 88]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = train_set[idx]['token_ids']\n",
    "res_list = [i for i, value in enumerate(token_ids) if value == 102]\n",
    "res_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>时间</th>\n",
       "      <th>锂电池需求量</th>\n",
       "      <th>锂电池YoY</th>\n",
       "      <th>三元电池需求量</th>\n",
       "      <th>三元电池YoY</th>\n",
       "      <th>磷酸铁锂&amp;钴酸锂需求量</th>\n",
       "      <th>磷酸铁锂&amp;钴酸锂YoY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016A</td>\n",
       "      <td>28.3</td>\n",
       "      <td>--</td>\n",
       "      <td>7.5</td>\n",
       "      <td>--</td>\n",
       "      <td>20.8</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017A</td>\n",
       "      <td>36.2</td>\n",
       "      <td>27.8%</td>\n",
       "      <td>15.8</td>\n",
       "      <td>111.9%</td>\n",
       "      <td>20.4</td>\n",
       "      <td>-2.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018E</td>\n",
       "      <td>48.3</td>\n",
       "      <td>33.6%</td>\n",
       "      <td>27.5</td>\n",
       "      <td>74.2%</td>\n",
       "      <td>20.8</td>\n",
       "      <td>2.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019E</td>\n",
       "      <td>73.8</td>\n",
       "      <td>52.7%</td>\n",
       "      <td>52.2</td>\n",
       "      <td>89.5%</td>\n",
       "      <td>21.6</td>\n",
       "      <td>4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020E</td>\n",
       "      <td>110.0</td>\n",
       "      <td>49.1%</td>\n",
       "      <td>88.6</td>\n",
       "      <td>69.9%</td>\n",
       "      <td>21.4</td>\n",
       "      <td>-1.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<__main__.Table at 0x7fc22ad84f28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[idx].table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "sel: [0]<br>agg: ['']<br>cond_conn_op: 'AND'<br>conds: [[1, '>', '30'], [3, '>', '10']]"
      ],
      "text/plain": [
       "sel: [0]\n",
       "agg: ['']\n",
       "cond_conn_op: 'AND'\n",
       "conds: [[1, '>', '30'], [3, '>', '10']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[idx].sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6639,  1.0065,  1.7779, -1.3565, -0.0894,  1.0683,  1.1525,  0.6744,\n",
       "        -0.3458,  0.1946])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([2,4,8])    # SEP list\n",
    "params  = torch.randn(10)          # params\n",
    "params\n",
    "\n",
    "#  XLS, a, SEP, b, SEP, c, c, c, SEP, pad, pad\n",
    "#  0  , 1, 2  , 3, 4  , 5, 6, 7, 8  , 9  , 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0065, 1.7779])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_positions = params[1:indices[0]+1]\n",
    "question_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-1.3565, -0.0894]), tensor([ 1.0683,  1.1525,  0.6744, -0.3458])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i, idx in enumerate(indices[:-1]):\n",
    "    id_start = idx+1   \n",
    "    id_end = indices[i+1]+1\n",
    "    result.append(params[id_start:id_end])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_column_positions(indice, params):\n",
    "    question_positions = params[1:indices[0]+1]\n",
    "    column_positions = []\n",
    "    for i, idx in enumerate(indices[:-1]):\n",
    "        id_start = idx+1   \n",
    "        id_end = indices[i+1]+1\n",
    "        column_positions.append(params[id_start:id_end])\n",
    "    return question_positions, column_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_seq_len = 9\n",
    "hidden_size = 6\n",
    "x = torch.empty(batch_size, max_seq_len, hidden_size)\n",
    "for i in range(batch_size):\n",
    "  for j in range(max_seq_len):\n",
    "    for k in range(hidden_size):\n",
    "      x[i,j,k] = i + j*10 + k*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_len = 20\n",
    "idx = 5 \n",
    "ones = np.ones(idx+1)\n",
    "zeros = np.zeros(max_len-idx-1)\n",
    "question_masks = np.concatenate([ones, zeros])\n",
    "question_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "max_len = 20\n",
    "idx = 5 \n",
    "ones = torch.ones(idx+1)\n",
    "zeros = torch.zeros(max_len-idx-1)\n",
    "question_masks = torch.cat([ones, zeros])\n",
    "question_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8063a049ce6126424592ab63ae068ec0717401a263e6cd39dbb78c77b7761238"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('transformers': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
