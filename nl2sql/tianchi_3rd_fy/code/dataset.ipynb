{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class Header:\n",
    "    def __init__(self, names: list, types: list):\n",
    "        self.names = names\n",
    "        self.types = types\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.names[idx], self.types[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ' | '.join(['{}({})'.format(n, t) for n, t in zip(self.names, self.types)])\n",
    "\n",
    "class Table:\n",
    "    def __init__(self, id, name, title, header: Header, rows, **kwargs):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.title = title\n",
    "        self.header = header\n",
    "        self.rows = rows\n",
    "        self._df = None\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        if self._df is None:\n",
    "            self._df = pd.DataFrame(data=self.rows,\n",
    "                                    columns=self.header.names,\n",
    "                                    dtype=str)\n",
    "        return self._df\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.df._repr_html_()\n",
    "\n",
    "class Tables:\n",
    "    table_dict = None\n",
    "\n",
    "    def __init__(self, table_list: list = None, table_dict: dict = None):\n",
    "        self.table_dict = {}\n",
    "        if isinstance(table_list, list):\n",
    "            for table in table_list:\n",
    "                self.table_dict[table.id] = table\n",
    "        if isinstance(table_dict, dict):\n",
    "            self.table_dict.update(table_dict)\n",
    "\n",
    "    def push(self, table):\n",
    "        self.table_dict[table.id] = table\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table_dict)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Tables(\n",
    "            table_list=list(self.table_dict.values()) +\n",
    "            list(other.table_dict.values())\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        return self.table_dict[id]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for table_id, table in self.table_dict.items():\n",
    "            yield table_id, table\n",
    "\n",
    "class Question:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "class SQL:\n",
    "    op_sql_dict = {0: \">\", 1: \"<\", 2: \"==\", 3: \"!=\"}\n",
    "    agg_sql_dict = {0: \"\", 1: \"AVG\", 2: \"MAX\", 3: \"MIN\", 4: \"COUNT\", 5: \"SUM\"}\n",
    "    conn_sql_dict = {0: \"NULL\", 1: \"AND\", 2: \"OR\"}\n",
    "\n",
    "    def __init__(self, cond_conn_op: int, agg: list, sel: list, conds: list, **kwargs):\n",
    "        self.cond_conn_op = cond_conn_op\n",
    "        self.sel = []\n",
    "        self.agg = []\n",
    "        sel_agg_pairs = zip(sel, agg)\n",
    "        sel_agg_pairs = sorted(sel_agg_pairs, key=lambda x: x[0])\n",
    "        for col_id, agg_op in sel_agg_pairs:\n",
    "            self.sel.append(col_id)\n",
    "            self.agg.append(agg_op)\n",
    "        self.conds = sorted(conds, key=lambda x: x[0])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        return cls(**data)\n",
    "\n",
    "    def keys(self):\n",
    "        return ['cond_conn_op', 'sel', 'agg', 'conds']\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(dict(self), ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "    def equal_all_mode(self, other):\n",
    "        return self.to_json() == other.to_json()\n",
    "\n",
    "    def equal_agg_mode(self, other):\n",
    "        self_sql = SQL(cond_conn_op=0, agg=self.agg, sel=self.sel, conds=[])\n",
    "        other_sql = SQL(cond_conn_op=0, agg=other.agg, sel=other.sel, conds=[])\n",
    "        return self_sql.to_json() == other_sql.to_json()\n",
    "\n",
    "    def equal_conn_and_agg_mode(self, other):\n",
    "        self_sql = SQL(cond_conn_op=self.cond_conn_op,\n",
    "                       agg=self.agg,\n",
    "                       sel=self.sel,\n",
    "                       conds=[])\n",
    "        other_sql = SQL(cond_conn_op=other.cond_conn_op,\n",
    "                        agg=other.agg,\n",
    "                        sel=other.sel,\n",
    "                        conds=[])\n",
    "        return self_sql.to_json() == other_sql.to_json()\n",
    "\n",
    "    def equal_no_val_mode(self, other):\n",
    "        self_sql = SQL(cond_conn_op=self.cond_conn_op,\n",
    "                       agg=self.agg,\n",
    "                       sel=self.sel,\n",
    "                       conds=[cond[:2] for cond in self.conds])\n",
    "        other_sql = SQL(cond_conn_op=other.cond_conn_op,\n",
    "                        agg=other.agg,\n",
    "                        sel=other.sel,\n",
    "                        conds=[cond[:2] for cond in other.conds])\n",
    "        return self_sql.to_json() == other_sql.to_json()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        raise NotImplementedError('compare mode not set')\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = ''\n",
    "        repr_str += \"sel: {}\\n\".format(self.sel)\n",
    "        repr_str += \"agg: {}\\n\".format([self.agg_sql_dict[a]\n",
    "                                        for a in self.agg])\n",
    "        repr_str += \"cond_conn_op: '{}'\\n\".format(\n",
    "            self.conn_sql_dict[self.cond_conn_op])\n",
    "        repr_str += \"conds: {}\".format(\n",
    "            [[cond[0], self.op_sql_dict[cond[1]], cond[2]] for cond in self.conds])\n",
    "\n",
    "        return repr_str\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.__repr__().replace('\\n', '<br>')\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, question: Question, table: Table, sql: SQL = None):\n",
    "        self.question = question\n",
    "        self.table = table\n",
    "        self.sql = sql\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        repr_str = '{}<br>{}<br>{}'.format(\n",
    "            self.table._repr_html_(),\n",
    "            self.question.__repr__(),\n",
    "            self.sql._repr_html_() if self.sql is not None else ''\n",
    "        )\n",
    "        return repr_str\n",
    "\n",
    "def read_tables(table_file):\n",
    "    tables = Tables()\n",
    "    with open(table_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tb = json.loads(line)\n",
    "            header = Header(tb.pop('header'), tb.pop('types'))\n",
    "            table = Table(header=header, **tb)\n",
    "            tables.push(table)\n",
    "    return tables\n",
    "\n",
    "def read_data(data_file, tables: Tables):\n",
    "    queries = []\n",
    "    with open(data_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            question = Question(text=data['question'])\n",
    "            table = tables[data['table_id']]\n",
    "            if 'sql' in data:\n",
    "                sql = SQL.from_dict(data['sql'])\n",
    "            else:\n",
    "                sql = None\n",
    "            query = Query(question=question, table=table, sql=sql)\n",
    "            queries.append(query)\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_file = '../../TableQA-master/train/train.tables.json'\n",
    "train_data_file = '../../TableQA-master/train/train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables = read_tables(train_table_file)\n",
    "train_data = read_data(train_data_file, train_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>影片名称</th>\n",
       "      <th>周票房（万）</th>\n",
       "      <th>票房占比（%）</th>\n",
       "      <th>场均人次</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>死侍2：我爱我家</td>\n",
       "      <td>10637.3</td>\n",
       "      <td>25.8</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>白蛇：缘起</td>\n",
       "      <td>10503.8</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大黄蜂</td>\n",
       "      <td>6426.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>密室逃生</td>\n",
       "      <td>5841.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“大”人物</td>\n",
       "      <td>3322.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>家和万事惊</td>\n",
       "      <td>635.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>钢铁飞龙之奥特曼崛起</td>\n",
       "      <td>595.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>海王</td>\n",
       "      <td>500.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>一条狗的回家路</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>掠食城市</td>\n",
       "      <td>356.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br>二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀<br>sel: [2]<br>agg: ['SUM']<br>cond_conn_op: 'OR'<br>conds: [[0, '==', '大黄蜂'], [0, '==', '密室逃生']]"
      ],
      "text/plain": [
       "<__main__.Query at 0x7fd29807a278>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('影片名称', 'text')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].table.header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(train_data[0].question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "sel: [4]<br>agg: ['SUM']<br>cond_conn_op: 'NULL'<br>conds: [[2, '==', '湖南卫视']]"
      ],
      "text/plain": [
       "sel: [4]\n",
       "agg: ['SUM']\n",
       "cond_conn_op: 'NULL'\n",
       "conds: [[2, '==', '湖南卫视']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 333\n",
    "train_data[idx].sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "影片名称(text) | 周票房（万）(real) | 票房占比（%）(real) | 场均人次(real)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].table.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('影片名称', 'text'), ('周票房（万）', 'real'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].table.header[0], train_data[0].table.header[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41522"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqlLabelEncoder:\n",
    "    \"\"\"\n",
    "    Convert SQL object into training labels.\n",
    "    \"\"\"\n",
    "    def encode(self, sql: SQL, num_cols):\n",
    "        cond_conn_op_label = sql.cond_conn_op\n",
    "        \n",
    "        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n",
    "        for col_id, agg_op in zip(sql.sel, sql.agg):\n",
    "            if col_id < num_cols:\n",
    "                sel_agg_label[col_id] = agg_op\n",
    "            \n",
    "        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n",
    "        for col_id, cond_op, _ in sql.conds:\n",
    "            if col_id < num_cols:\n",
    "                cond_op_label[col_id] = cond_op\n",
    "            \n",
    "        return cond_conn_op_label, sel_agg_label, cond_op_label\n",
    "    \n",
    "    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n",
    "        cond_conn_op = int(cond_conn_op_label)\n",
    "        sel, agg, conds = [], [], []\n",
    "\n",
    "        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n",
    "            if agg_op < len(SQL.agg_sql_dict):\n",
    "                sel.append(col_id)\n",
    "                agg.append(int(agg_op))\n",
    "            if cond_op < len(SQL.op_sql_dict):\n",
    "                conds.append([col_id, int(cond_op)])\n",
    "        return {\n",
    "            'sel': sel,\n",
    "            'agg': agg,\n",
    "            'cond_conn_op': cond_conn_op,\n",
    "            'conds': conds\n",
    "        }\n",
    "\n",
    "sql_le = SqlLabelEncoder()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1),\n",
       " tensor([6, 4, 6, 6, 6, 6, 6, 6], dtype=torch.int32),\n",
       " tensor([4, 4, 4, 4, 4, 4, 2, 2], dtype=torch.int32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx= 3\n",
    "cond_conn_op, sel_agg, cond_op = sql_le.encode(train_data[idx].sql, num_cols=len(train_data[idx].table.header))\n",
    "torch.tensor(cond_conn_op), torch.tensor(sel_agg), torch.tensor(cond_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from utils import SQL\n",
    "\n",
    "import config\n",
    "\n",
    "class SqlLabelEncoder:\n",
    "    \"\"\"\n",
    "    Convert SQL object into training labels.\n",
    "    \"\"\"\n",
    "    def encode(self, sql: SQL, num_cols):\n",
    "        cond_conn_op_label = sql.cond_conn_op\n",
    "        \n",
    "        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n",
    "        for col_id, agg_op in zip(sql.sel, sql.agg):\n",
    "            if col_id < num_cols:\n",
    "                sel_agg_label[col_id] = agg_op\n",
    "            \n",
    "        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n",
    "        for col_id, cond_op, _ in sql.conds:\n",
    "            if col_id < num_cols:\n",
    "                cond_op_label[col_id] = cond_op\n",
    "            \n",
    "        return cond_conn_op_label, sel_agg_label, cond_op_label\n",
    "    \n",
    "    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n",
    "        cond_conn_op = int(cond_conn_op_label)\n",
    "        sel, agg, conds = [], [], []\n",
    "\n",
    "        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n",
    "            if agg_op < len(SQL.agg_sql_dict):\n",
    "                sel.append(col_id)\n",
    "                agg.append(int(agg_op))\n",
    "            if cond_op < len(SQL.op_sql_dict):\n",
    "                conds.append([col_id, int(cond_op)])\n",
    "        return {\n",
    "            'sel': sel,\n",
    "            'agg': agg,\n",
    "            'cond_conn_op': cond_conn_op,\n",
    "            'conds': conds\n",
    "        }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    # customized dataset\n",
    "    # implement ___len___ & __getitem__ function\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        sql_label_encoder=SqlLabelEncoder(), \n",
    "        max_len=config.MAX_LEN, \n",
    "        model_name=config.BASE_MODEL_PATH, \n",
    "        SEP_temp='|', \n",
    "        REAL_temp = '?',\n",
    "        TEXT_temp = '!',\n",
    "        cls_token='[CLS]', \n",
    "        REAL_token='[unused20]', \n",
    "        TEXT_token='[unused21]'\n",
    "    ):\n",
    "        self.data = data                                                                     # loaded data\n",
    "        self.max_len = max_len                                                               # max length of sequence\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cls_token=cls_token)      # cls_token will be replaced by a unused token in bert vocab \n",
    "        self.indexes = np.arange(len(self.data))                                             # set a list of indexes according to data length\n",
    "        self.sql_label_encoder = sql_label_encoder                                           # label encoder for SQL objects\n",
    "\n",
    "        self.CLS_id = self.tokenizer.encode([cls_token])[1]                                  # CLS's token id \n",
    "        self.REAL_id = self.tokenizer.encode([REAL_token])[1]                                # REAL's token id \n",
    "        self.TEXT_id = self.tokenizer.encode([TEXT_token])[1]                                # TEXT's token id \n",
    "        self.SEP_id = self.tokenizer.encode('[SEP]')[1]                                      # SEP's token id \n",
    "\n",
    "        self.SEP_temp = SEP_temp                                                             # temporarily symbol for SEP token\n",
    "        self.REAL_temp = REAL_temp                                                           # temporarily symbol for REAL token\n",
    "        self.TEXT_temp = TEXT_temp                                                           # temporarily symbol for TEXT token      \n",
    "\n",
    "        self.SEP_temp_id = self.tokenizer.encode(SEP_temp)[1]                                # SEP_temp's token id \n",
    "        self.REAL_temp_id = self.tokenizer.encode(REAL_temp)[1]                              # REAL_temp's token id \n",
    "        self.TEXT_temp_id = self.tokenizer.encode(TEXT_temp)[1]                              # TEXT_temp's token id \n",
    "\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        question = str(self.data[idx].question)\n",
    "\n",
    "        # construct header in str\n",
    "        header_sent = ''\n",
    "        for header, label in self.data[idx].table.header:\n",
    "            if label == 'text':\n",
    "                header_sent += self.TEXT_temp\n",
    "            if label == 'real':\n",
    "                header_sent += self.REAL_temp\n",
    "            header_sent += re.sub(r'[\\(\\（].*[\\)\\）]', '', header)   # remove brackets for headers\n",
    "            header_sent += self.SEP_temp\n",
    "        header_sent = header_sent[:-1]\n",
    "\n",
    "        # print(question)\n",
    "        # print(header_sent)\n",
    "        # print(self.XLS_id)\n",
    "        # print(self.SEP_id)\n",
    "        # print(self.REAL_id)\n",
    "        # print(self.TEXT_id)\n",
    "\n",
    "        embeddings = self.tokenizer(\n",
    "            question, header_sent,                    # sentence 1, sentence 2\n",
    "            padding='max_length',                     # Pad to max_length\n",
    "            truncation=True,                          # Truncate to max_length\n",
    "            max_length=self.max_len,                  # Set max_length\n",
    "            return_tensors='pt'                       # Return torch.Tensor objects\n",
    "        )\n",
    "\n",
    "        token_ids = torch.squeeze(embeddings['input_ids'])                                               # tensor of token ids\n",
    "        token_ids = torch.where(token_ids==self.SEP_temp_id, torch.tensor(self.SEP_id), token_ids)       # replace SEP_temp_id by SEP_id\n",
    "        token_ids = torch.where(token_ids==self.REAL_temp_id, torch.tensor(self.REAL_id), token_ids)     # replace REAL_temp_id by REAL_id\n",
    "        token_ids = torch.where(token_ids==self.TEXT_temp_id, torch.tensor(self.TEXT_id), token_ids)     # replace TEXT_temp_id by TEXT_id\n",
    "\n",
    "        attention_masks = torch.squeeze(embeddings['attention_mask'])                                    # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = torch.squeeze(embeddings['token_type_ids'])                                     # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        header_ids = [i for i, value in enumerate(token_ids) if value == self.REAL_id or value == self.TEXT_id]                       # list of SEP positions, length: nb of cols + 1\n",
    "\n",
    "        # True if the dataset has labels (when training or validating or testing)\n",
    "        if self.sql_label_encoder is not None:       \n",
    "            COND_CONN_OP, SEL_AGG, COND_OP = self.sql_label_encoder.encode(self.data[idx].sql, num_cols=len(self.data[idx].table.header))             \n",
    "            # COND_CONN_OP = self.data[idx].sql['cond_conn_op']\n",
    "            return {\n",
    "                ### X\n",
    "                'token_ids': token_ids.to(torch.int32),\n",
    "                'token_type_ids': token_type_ids.to(torch.int32),\n",
    "                'attention_masks': attention_masks.to(torch.int32),\n",
    "                'header_ids': torch.tensor(header_ids, dtype=torch.int32),\n",
    "                'header_masks': torch.ones(len(header_ids), dtype=torch.int32),\n",
    "                ### y\n",
    "                'COND_CONN_OP': torch.tensor(COND_CONN_OP, dtype=torch.int32),\n",
    "                'SEL_AGG': torch.tensor(SEL_AGG, dtype=torch.int32),\n",
    "                'COND_OP': torch.tensor(COND_OP, dtype=torch.int32)         \n",
    "            } \n",
    "        # False if the dataset do not have labels (when inferencing)\n",
    "        else:                                                           \n",
    "            return {\n",
    "                'token_ids': token_ids.to(torch.int32),\n",
    "                'token_type_ids': token_type_ids.to(torch.int32),\n",
    "                'attention_masks': attention_masks.to(torch.int32),\n",
    "                'header_ids_ids': torch.tensor(header_ids, dtype=torch.int32),\n",
    "                'header_ids_masks': torch.ones(len(header_ids), dtype=torch.int32),\n",
    "            }\n",
    "            \n",
    "\n",
    "def collate_fn(batch_data):\n",
    "    '''\n",
    "    SEP_ids lengths are different in each batch, need padding\n",
    "    '''\n",
    "    batch_data.sort(key=lambda xi: len(xi['header_ids']), reverse=True)\n",
    "\n",
    "    header_ids_seq = [xi['header_ids'] for xi in batch_data]\n",
    "    padded_header_ids_seq = torch.nn.utils.rnn.pad_sequence(header_ids_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    header_masks_seq = [xi['header_masks'] for xi in batch_data]\n",
    "    padded_header_masks_seq = torch.nn.utils.rnn.pad_sequence(header_masks_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    COND_CONN_OP_seq = [xi['COND_CONN_OP'] for xi in batch_data]\n",
    "\n",
    "    SEL_AGG_seq = [xi['SEL_AGG'] for xi in batch_data]\n",
    "    padded_SEL_AGG_seq = torch.nn.utils.rnn.pad_sequence(SEL_AGG_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    COND_OP_seq = [xi['COND_OP'] for xi in batch_data]\n",
    "    padded_COND_OP_seq = torch.nn.utils.rnn.pad_sequence(COND_OP_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    token_ids_seq = [xi['token_ids'] for xi in batch_data]\n",
    "    token_type_ids_seq = [xi['token_type_ids'] for xi in batch_data]\n",
    "    attention_masks_seq = [xi['attention_masks'] for xi in batch_data]\n",
    "\n",
    "\n",
    "    return {\n",
    "        ### X\n",
    "        'token_ids': torch.stack(token_ids_seq) , \n",
    "        'token_type_ids': torch.stack(token_type_ids_seq),\n",
    "        'attention_masks': torch.stack(attention_masks_seq),\n",
    "        'header_ids': padded_header_ids_seq,\n",
    "        'header_masks': padded_header_masks_seq,\n",
    "        ### y\n",
    "        'COND_CONN_OP': torch.stack(COND_CONN_OP_seq),\n",
    "        'COND_OP': padded_SEL_AGG_seq,\n",
    "        'SEL_AGG': padded_COND_OP_seq\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn_labelless(batch_data):\n",
    "    '''\n",
    "    SEP_ids lengths are different in each batch, need padding\n",
    "    For no label case\n",
    "    '''\n",
    "    batch_data.sort(key=lambda xi: len(xi['header_ids']), reverse=True)\n",
    "    header_ids_seq = [xi['header_ids'] for xi in batch_data]\n",
    "    padded_header_ids_seq = torch.nn.utils.rnn.pad_sequence(header_ids_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    header_masks_seq = [xi['header_masks'] for xi in batch_data]\n",
    "    padded_header_masks_seq = torch.nn.utils.rnn.pad_sequence(header_masks_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "    token_ids_seq = [xi['token_ids'] for xi in batch_data]\n",
    "    token_type_ids_seq = [xi['token_type_ids'] for xi in batch_data]\n",
    "    attention_masks_seq = [xi['attention_masks'] for xi in batch_data]\n",
    "\n",
    "    return {\n",
    "        ### X\n",
    "        'token_ids': torch.stack(token_ids_seq) , \n",
    "        'token_type_ids': torch.stack(token_type_ids_seq),\n",
    "        'attention_masks': torch.stack(attention_masks_seq),\n",
    "        'header_ids': padded_header_ids_seq,\n",
    "        'header_masks': padded_header_masks_seq,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([ 101, 2791, 1765,  772, 2458, 1355, 2832, 6598, 1398, 3683, 1872, 6862,\n",
       "         1469, 6589, 4500, 1398, 3683, 1872, 6862, 6963, 1920,  754,  124,  110,\n",
       "         4638, 3221, 1525, 1126, 2399,  102,   21, 3198, 7313,  102,   20, 2791,\n",
       "         1765,  772, 2458, 1355, 2832, 6598, 2130, 2768, 7583,  102,   20, 2832,\n",
       "         6598, 1398, 3683, 1872, 6862,  102,   20, 1071,  800, 6589, 4500,  102,\n",
       "           20, 1071, 2124, 6589, 4500, 1872, 6862,  102,   20, 2456, 2128, 2832,\n",
       "         6598,  102,   20, 6589, 4500, 1398, 3683, 1872, 6862,  102,   21, 3177,\n",
       "         2339, 7481, 2832, 6598, 2130, 2768, 7583,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'attention_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'header_ids': tensor([30, 34, 46, 54, 60, 68, 74, 82], dtype=torch.int32),\n",
       " 'header_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       " 'COND_CONN_OP': tensor(1, dtype=torch.int32),\n",
       " 'SEL_AGG': tensor([0, 6, 6, 6, 6, 6, 6, 6], dtype=torch.int32),\n",
       " 'COND_OP': tensor([4, 4, 0, 4, 4, 4, 0, 4], dtype=torch.int32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[2332]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = CustomDataset(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_set, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([[ 101, 1525,  763, 7770, 6862, 8126, 2399, 4638, 5500, 2622, 4372, 2207,\n",
      "          754,  127,  110, 8024, 1398, 3198, 1071, 2094, 6121,  689, 1348, 3221,\n",
      "          784,  720,  102,   21, 3403, 4638,  102,   21, 2094, 6121,  689,  102,\n",
      "           20, 8109, 5500, 2622, 4372,  102,   20, 8112, 5500, 2622, 4372,  102,\n",
      "           20, 8119, 5500, 2622, 4372,  102,   20, 8109, 1146, 5273, 3683,  891,\n",
      "          102,   20, 8112, 1146, 5273, 3683,  891,  102,   20, 8119, 1146, 5273,\n",
      "         3683,  891,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=torch.int32), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), 'attention_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), 'header_ids': tensor([[27, 31, 36, 42, 48, 54, 61, 68]], dtype=torch.int32), 'header_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32), 'COND_CONN_OP': tensor([0], dtype=torch.int32), 'COND_OP': tensor([[0, 0, 6, 6, 6, 6, 6, 6]], dtype=torch.int32), 'SEL_AGG': tensor([[4, 4, 1, 4, 4, 4, 4, 4]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8063a049ce6126424592ab63ae068ec0717401a263e6cd39dbb78c77b7761238"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('transformers': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
