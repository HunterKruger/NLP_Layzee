{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import config\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import ast\n",
    "import datetime\n",
    "import jieba\n",
    "import emoji\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import jieba.posseg as pseg\n",
    "from zhconv import convert\n",
    "from time import time\n",
    "from datetime import date\n",
    "from collections import Counter\n",
    "from category_encoders.count import CountEncoder\n",
    "from datetime import timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from functools import reduce \n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取参数\n"
     ]
    }
   ],
   "source": [
    "print('读取参数')\n",
    "cn_stopword_path = config.cn_stopword_path\n",
    "en_stopword_path = config.en_stopword_path\n",
    "common_stopword_path = config.common_stopword_path\n",
    "keepword_path = config.keepword_path\n",
    "synonyms_path = config.synonyms_path\n",
    "legal_pos = config.legal_pos\n",
    "tokenized_filename = config.tokenized_filename\n",
    "\n",
    "tfidf_max_df = config.tfidf_max_df\n",
    "tfidf_max_features = config.tfidf_max_features\n",
    "tfidf_top_k = config.tfidf_top_k\n",
    "\n",
    "bigrams_count = config.bigrams_count\n",
    "bigrams_occur = config.bigrams_occur\n",
    "bigrams_window = config.bigrams_window\n",
    "bigrams_show_samples = config.bigrams_show_samples\n",
    "bigrams_time_limit = config.bigrams_time_limit\n",
    "bigrams_result_df_path = config.bigrams_result_df_path\n",
    "\n",
    "trigrams_count = config.trigrams_count\n",
    "trigrams_occur = config.trigrams_occur\n",
    "trigrams_window = config.trigrams_window\n",
    "trigrams_show_samples = config.trigrams_show_samples\n",
    "trigrams_time_limit = config.trigrams_time_limit\n",
    "trigrams_result_df_path = config.trigrams_result_df_path\n",
    "\n",
    "standard_Q_path = config.standard_Q_path\n",
    "config_path = config.config_path\n",
    "\n",
    "tokenized_tablename = config.tokenized_tablename\n",
    "bigram_tablename = config.bigram_tablename\n",
    "trigram_tablename = config.trigram_tablename\n",
    "\n",
    "adm_tokenized_tablename = config.adm_tokenized_tablename\n",
    "adm_bigram_tablename = config.adm_bigram_tablename\n",
    "adm_trigram_tablename = config.adm_trigram_tablename\n",
    "\n",
    "input_filename = config.input_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从数据库拉取一周数据:\n",
      "27.54895257949829\n"
     ]
    }
   ],
   "source": [
    "print('从数据库拉取一周数据:')\n",
    "t1 = time()\n",
    "# hive syntax\n",
    "os.system(\n",
    "    f\"\"\"\n",
    "        hive -e \"\n",
    "            select * from tmp_htl_ai_db.tmp_fy_im_weekly\n",
    "        \">{input_filename}\n",
    "    \"\"\"\n",
    ")\n",
    "t2 = time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 40492: Expected 6 fields in line 40492, saw 7\n",
      "Skipping line 52123: Expected 6 fields in line 52123, saw 8\n",
      "Skipping line 66938: Expected 6 fields in line 66938, saw 7\n",
      "Skipping line 67063: Expected 6 fields in line 67063, saw 9\n",
      "Skipping line 142778: Expected 6 fields in line 142778, saw 8\n",
      "Skipping line 178960: Expected 6 fields in line 178960, saw 8\n",
      "Skipping line 217510: Expected 6 fields in line 217510, saw 11\n",
      "Skipping line 264890: Expected 6 fields in line 264890, saw 7\n",
      "Skipping line 301657: Expected 6 fields in line 301657, saw 11\n",
      "Skipping line 301661: Expected 6 fields in line 301661, saw 11\n",
      "Skipping line 307824: Expected 6 fields in line 307824, saw 7\n",
      "Skipping line 371546: Expected 6 fields in line 371546, saw 7\n",
      "Skipping line 418162: Expected 6 fields in line 418162, saw 9\n",
      "Skipping line 451972: Expected 6 fields in line 451972, saw 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集shape:  (454287, 6)\n",
      "1. 清洗和筛选\n",
      "对session_id进行计数编码\n",
      "总session数量 38492\n",
      "1.1 round完整性筛选\n",
      "有rank1的session数量： (38232,)\n",
      "有rank1的session比例： 0.993245349683051\n",
      "rank=round的session数量 (38242,)\n",
      "rank=round的session比例 0.9935051439260106\n",
      "完整的session数量： (38220, 1)\n",
      "完整的session占比： 0.9929335965914995\n",
      "1.2 根据round内部发言筛选\n",
      "聚合type字段\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/powerop/work/conda/envs/IM_high_freq/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "确保session中同时有顾客客服和机器人的发言\n",
      "此时的session数量 (28497, 3)\n",
      "取最后的顾客发言之前的一个机器发言\n",
      "此时数据集shape:  (262233, 10)\n",
      "1.3 根据type和msgtype筛选\n",
      "此时数据集shape:  (104126, 10)\n",
      "1.4 JSON 转文本\n",
      "1.5 Session聚合\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此时数据集shape:  (28444, 2)\n",
      "删除转义符，防止保存csv的时候出错\n",
      "2. 分词\n",
      "2.1 读取停止词\n",
      "停止词数量： 903\n",
      "2.2 加载保留词\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.971 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2~ 加载同义词\n",
      "同义词对数量： 23\n",
      "2.2 开始分词\n",
      "当前数据量 (25375, 4)\n",
      "保存分词后的数据\n",
      "3. TF-IDF建模：\n",
      "TFIDF后的数据大小 (25375, 1000)\n",
      "Topwords:\n",
      "['a', 'app', 'fi', 'h', 'ktv', 'loft', 'qq', 't2', 'v', 'vip', 'wi', 'wifi', '一定', '一小', '一楼', '一次性', '一直', '三人间', '三房', '三间房', '上班', '上订', '下午茶', '下单', '下好', '下班', '下车', '下雨', '不上', '不住', '不可', '不同', '不够', '不好', '不对', '不敢', '不能取消', '不让', '不远', '不通', '不错', '专票', '丢', '两张床', '两间房', '严重', '个人', '中心', '中风险', '久', '乐园', '买', '买票', '了有', '事', '事情', '二楼', '交', '交通', '享受', '亲亲', '亲子', '亲子房', '人员', '人间', '介绍', '付', '付款', '付费', '付钱', '价', '价位', '价格', '价钱', '休息', '优惠', '会员', '会议室', '估计', '位置', '低', '低风险', '住', '住客', '住宿', '住店', '住房', '体验', '使用', '便宜', '保留', '保证', '信号', '信息', '修改', '做', '做好', '做核酸', '做饭', '停', '停电', '停车', '停车位', '停车场', '停车费', '健康', '健康码', '健身房', '儿童', '儿童乐园', '允许', '充电', '充电器', '先', '先定', '免费', '兑换', '入', '入境', '入驻', '全', '全款', '全额', '公众', '公司', '公寓', '公用', '关', '关系', '关闭', '其实', '具体', '具体位置', '内容', '再定', '再订', '写', '冰箱', '决定', '冷', '准备', '几楼', '出', '出不来', '出京', '出入', '出去', '出发', '出差', '出现', '出示', '出行', '出过', '出门', '分', '分开', '刚定', '删除', '别墅', '到底', '到时候', '到达', '刷', '刷卡', '前台', '剩', '剩下', '办', '办事', '办法', '办理', '加', '加床', '加钱', '包', '包含', '包括', '包月', '区', '区别', '区域', '医院', '升级', '升级成', '午餐', '协调', '单', '单人', '单人床', '单人间', '单位', '单独', '单间', '卖', '卡', '卧', '卧室', '卫生间', '即可', '厅', '原因', '原来', '厨具', '厨房', '参加', '双', '双人', '双人床', '双人间', '双床', '双床房', '反应', '反正', '发', '发现', '发票', '发给', '发给你', '取', '取消', '变', '变化', '变成', '只能', '可否', '可能', '号码', '司机', '吃', '吃饭', '合作', '合肥', '合适', '同事', '同意', '名字', '名称', '含', '听', '听说', '吵', '呆', '告知', '告诉', '员工', '呜呜', '咨询', '商务', '商家', '啥意思', '喜欢', '回', '回去', '回复', '回家', '回来', '回答', '园区', '国际', '图', '图片', '地区', '地图', '地址', '地方', '地点', '地铁', '地铁站', '坐', '坐飞机', '垃圾', '城区', '填', '增值税', '声音', '处理', '备注', '复式', '外出', '外卖', '外国人', '外地', '外省', '外籍', '多长时间', '大人', '大厦', '大堂', '大床', '大床房', '大概', '大理', '大约', '大连', '天', '天气', '太', '太好了', '太晚', '套房', '套餐', '女朋友', '女生', '好了吗', '好像', '好看', '好评', '妈', '姐姐', '姓', '姓名', '娱乐', '学校', '学生', '学院', '孩子', '安排', '完', '完成', '定', '定位', '定好', '定错', '宝', '实在', '实际', '宠物', '客人', '客厅', '客户', '客房', '客栈', '室', '害怕', '家庭', '家长', '宾馆', '寄', '寄存', '密码', '对面', '导航', '小区', '小孩', '小时', '小朋友', '小院', '少', '就行了', '尽快', '尽量', '居', '居住', '居家', '属于', '山', '山景', '岛', '工作', '工作人员', '工具', '差', '差不多', '差价', '已经', '市', '市区', '布置', '希望', '帐篷', '带', '带星号', '带有', '帮', '帮忙', '干净', '平', '平台', '广场', '床', '床单', '床垫', '应该', '店', '店家', '店里', '店长', '庭院', '康码', '延期', '延迟', '延长', '建议', '开', '开业', '开会', '开具', '开发票', '开户行', '开房', '开放', '开票', '开车', '开门', '弄', '弹窗', '影响', '影院', '影音', '得到', '微', '微信', '必须', '忘', '忘记', '忙', '快', '快点', '快递', '怎么回事', '怎么弄', '怕', '急', '恢复', '情侣', '情况', '想住', '想定', '想要', '想订', '意思', '感觉', '感谢您', '懂', '成', '成人', '成功', '成年人', '我会', '截图', '户型', '户籍', '房', '房价', '房卡', '房号', '房型', '房子', '房费', '房车', '房间', '房间内', '手机', '手续', '手续费', '打不', '打印', '打开', '打扫', '打扫卫生', '打扰', '打电话', '打算', '打车', '打通', '扣', '扣款', '扣费', '扣钱', '扫', '找', '找到', '投屏', '投影', '投影仪', '投诉', '抖音', '折扣', '护照', '报', '报告', '报备', '报销', '抬头', '押金', '担心', '拉', '拍', '拍照', '拖鞋', '招', '拼', '拿到', '持', '持有', '挂', '指', '指定', '挨着', '换', '换成', '换房', '掉', '接', '接到', '接受', '接待', '接收', '接机', '接电话', '接站', '接送', '接送机', '控', '推荐', '推迟', '措施', '提交', '提供', '提前', '提示', '搞', '携带', '摘星', '操作', '支付', '支持', '支行', '收', '收到', '收取', '收拾', '收费', '改', '改成', '改期', '放', '放假', '放在', '放心', '政府', '政策', '效果', '新', '新增', '方便', '方式', '方面', '旅客', '旅游', '无法', '日出', '早', '早点', '早餐', '时', '时间', '时间段', '星', '星号', '星星', '是不是', '是从', '是否', '是订', '显示', '晚一点', '晚安', '晚点', '晚餐', '普票', '普通', '景区', '景点', '景观', '智能', '暂时', '更', '更好', '更改', '最低', '最大', '最好', '最新', '最早', '最迟', '最高', '月份', '月租', '有变', '有无', '有点', '有限公司', '朋友', '服务', '未', '未成年', '未成年人', '未满', '本地人', '本市', '机场', '机票', '权益', '条件', '来不及', '来住', '查', '查到', '查看', '查询', '标准', '标间', '样子', '核实', '核算', '核酸', '核酸报告', '核酸检测', '桌子', '检', '检查', '检测', '棋牌', '楼', '楼层', '榻榻米', '正常', '步行', '死', '比较', '毕竟', '民宿', '水', '水单', '江景', '汤', '沙发', '沟通', '没', '没了', '没人接', '没什么', '没住', '没出', '没到', '没去', '没定', '没开', '没房', '没收', '没法', '没订', '没退', '泡', '泡池', '注意', '泳池', '洗手间', '洗漱', '洗澡', '洗衣', '洗衣房', '洗衣服', '洗衣机', '活动', '浴室', '浴缸', '海', '海景', '海景房', '海珠', '消息', '消毒', '消费', '涨价', '清楚', '游客', '游泳', '游泳池', '湖景', '湾', '滑梯', '满', '滴', '火车', '火车站', '炉子', '点评', '烟台', '烧烤', '热', '热水', '照片', '煮', '父母', '特价', '特别', '特殊', '狗狗', '独立', '玩', '环境', '现场', '理解', '用品', '用餐', '申请', '电', '电子', '电子版', '电影', '电梯', '电竞房', '电脑', '电视', '电视机', '电话', '电话号码', '男朋友', '留', '疫情', '病例', '登记', '白色', '的对', '监护人', '直达', '相关', '看下', '看不到', '看到', '看有', '看看', '看着', '看见', '真', '真的', '着急', '睡', '睡觉', '知道', '短信', '短租', '码', '码头', '确定', '确认', '碳', '礼遇', '社区', '票', '离店', '离开', '私人', '私汤', '租', '稍', '稍微', '税号', '税点', '稳定', '空', '空房', '空调', '空间', '窗户', '站', '竞', '答复', '简单', '算', '管控', '管理', '类型', '精品', '精致', '系统', '约', '纸质', '线', '经常', '经理', '结束', '给我发', '统一', '继续', '续', '续住', '续房', '续订', '续费', '绿', '绿码', '绿色', '网络', '网速', '美团', '老', '老人', '考虑', '考试', '联系', '联系方式', '联系电话', '肯定', '能住', '能开', '能用', '能订', '自动', '自助', '自助餐', '自带', '自由', '自费', '自驾', '舒服', '舒适', '航班', '船', '花', '花园', '营业', '落', '落地', '落地窗', '蓝色', '蛋糕', '行不行', '行政', '行李', '行程', '行程码', '街道', '衣服', '补', '被子', '装修', '要住', '要定', '要求', '要订', '见', '观景', '规定', '视频', '觉得', '解决', '解封', '解除', '计划', '订', '订单', '订单号', '订好', '订完', '订房', '订房间', '订购', '订错', '记录', '记得', '设备', '设施', '证件', '证明', '评价', '评论', '识别', '询问', '说', '说话', '请', '调整', '谢', '豪华', '账', '账号', '购买', '贵', '贸易', '费', '费用', '资料', '赠送', '走', '走路', '起来', '超过', '距离', '路', '身份证', '车', '车位', '车站', '转', '转机', '近', '返回', '进', '进入', '进出', '进去', '进来', '进行', '远', '连住', '连续', '退', '退回', '退房', '退掉', '退款', '退订', '送', '送到', '送机', '适合', '选', '选择', '选项', '透明', '途径', '通知', '遇到', '那就好', '邮寄', '邮箱', '部门', '配置', '酒店', '酒廊', '重新', '金额', '钟点', '钱', '银行', '链接', '锅', '错', '长', '长住', '长期', '长租', '门', '门票', '闪住', '问下', '间房', '防控', '防疫', '防范', '阳台', '阴性', '限制', '院', '院子', '随便', '随机', '隔离', '隔音', '难', '雅致', '集中', '需', '需不需要', '需要', '露台', '露营', '非常', '靠近', '面积', '页面', '项目', '预售', '预定', '预留', '预约', '预计', '预订', '额', '风景', '风险', '飞', '飞机', '食材', '餐厅', '马', '马桶', '马路', '高', '高层', '高速', '高铁', '高铁站', '高风险', '麻将', '麻将桌', '黄码']\n",
      "3.1 提取每个session的关键词：\n",
      "预处理耗时： 106.19233989715576\n"
     ]
    }
   ],
   "source": [
    "t1=time()\n",
    "\n",
    "df=pd.read_csv(input_filename, header=0,sep='\\t', error_bad_lines=False, engine='python')\n",
    "df = df[['session_id','datachange_createtime',  'msgtype', 'type', 'rank', 'body']]\n",
    "print('原始数据集shape: ', df.shape)\n",
    "\n",
    "print('1. 清洗和筛选')\n",
    "\n",
    "print('对session_id进行计数编码')\n",
    "df_session = df['session_id']\n",
    "ce = CountEncoder(cols=['session_id'])\n",
    "df_encoded = ce.fit_transform(df)\n",
    "df_encoded.columns = ['round','datachange_createtime',  'msgtype', 'type', 'rank', 'body']\n",
    "df = pd.concat([df_session, df_encoded], axis=1)\n",
    "\n",
    "session_ids = df['session_id'].drop_duplicates()\n",
    "session_ids_count = session_ids.shape[0]\n",
    "print('总session数量', session_ids_count)\n",
    "\n",
    "print('1.1 round完整性筛选')\n",
    "rank1_session_ids = df[df['rank']==1]['session_id'].drop_duplicates()\n",
    "print('有rank1的session数量：', rank1_session_ids.shape)\n",
    "print('有rank1的session比例：', rank1_session_ids.shape[0] / session_ids.shape[0])   \n",
    "\n",
    "rank_eq_round_session_ids = df[df['rank']==df['round']]['session_id'].drop_duplicates()\n",
    "print('rank=round的session数量', rank_eq_round_session_ids.shape)\n",
    "print('rank=round的session比例', rank_eq_round_session_ids.shape[0] / session_ids.shape[0] )       \n",
    "\n",
    "session_ids = pd.merge(rank1_session_ids, session_ids, how='inner', on='session_id')\n",
    "session_ids = pd.merge(rank_eq_round_session_ids, session_ids, how='inner', on='session_id')\n",
    "print('完整的session数量：', session_ids.shape)\n",
    "print('完整的session占比：', session_ids.shape[0] / session_ids_count)  # session 既完整也能进人工 比例\n",
    "\n",
    "df = pd.merge(session_ids, df, how='left', on='session_id')\n",
    "\n",
    "print('1.2 根据round内部发言筛选')\n",
    "print('聚合type字段')\n",
    "df_temp = df[['session_id', 'type']]\n",
    "df_temp['type'] = df_temp['type'].apply(lambda x: str(x))\n",
    "df_temp = df_temp.groupby(by='session_id')['type'].sum().to_frame()  # 聚合type\n",
    "df_temp = df_temp.reset_index()\n",
    "df_temp.columns = ['session_id', 'type_seq']\n",
    "\n",
    "print('确保session中同时有顾客客服和机器人的发言')\n",
    "def find_man_and_machine(x):  \n",
    "    if ('1' in x and '2' in x and '3' in x) or ('1' in x and '2' in x and '4' in x):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_temp['man_and_machine'] = df_temp['type_seq'].apply(lambda x: find_man_and_machine(x))\n",
    "df_temp = df_temp[df_temp['man_and_machine']==True]\n",
    "print('此时的session数量', df_temp.shape)\n",
    "\n",
    "\n",
    "print('取最后的顾客发言之前的一个机器发言')\n",
    "def find_last_machine(x):\n",
    "    last1idx = len(x) - x[::-1].index('1') - 1\n",
    "    x = x[:last1idx]\n",
    "    if '2' in x:\n",
    "        return len(x) - x[::-1].index('2') \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_temp['last_machine'] = df_temp['type_seq'].apply(lambda x: find_last_machine(x))\n",
    "df = pd.merge(df_temp, df, how='left', on='session_id')\n",
    "df = df[df['rank'] > df['last_machine']]\n",
    "print('此时数据集shape: ', df.shape)\n",
    "\n",
    "print('1.3 根据type和msgtype筛选')\n",
    "type_dict = {1:'客人',2:'机器',3:\"酒店客服人员\",4:'携程客服人员', 5:'服务评价'}\n",
    "msgtype_dict = {0:'手打文本',1:'图像',2:'酒店或房型',3:'视频',4:'语音',5:'文件或链接',6:'坐标',7:'json模板'} \n",
    "df = df[(df['type']==1) ]\n",
    "df = df[(df['msgtype']==0) | (df['msgtype']==7)]\n",
    "print('此时数据集shape: ', df.shape)\n",
    "\n",
    "print('1.4 JSON 转文本')\n",
    "def get_title(body):\n",
    "    if 'title' in body and '{' in body and '}' in body:\n",
    "        return json.loads(body, strict=False)['title']\n",
    "    else:\n",
    "        return body\n",
    "df['body']=df['body'].apply(lambda x : get_title(x))\n",
    "\n",
    "print('1.5 Session聚合')\n",
    "df = df[['session_id','rank','body']]\n",
    "df = df.groupby(by='session_id').body.sum().to_frame()  # 聚合body\n",
    "df = df.reset_index()\n",
    "print('此时数据集shape: ', df.shape)\n",
    "\n",
    "print('删除转义符，防止保存csv的时候出错')\n",
    "df['body'] = df['body'].apply(lambda x: x.replace('\\r',''))  \n",
    "df['body'] = df['body'].apply(lambda x: x.replace('\\n',''))  \n",
    "df['body'] = df['body'].apply(lambda x: x.replace('\\t',''))  \n",
    "\n",
    "print('2. 分词')\n",
    "print('2.1 读取停止词')\n",
    "f = open(cn_stopword_path,\"r\")   \n",
    "stop_sents_cn = f.read()    \n",
    "f.close()   \n",
    "stop_sents_cn = stop_sents_cn.split('\\n')\n",
    "f = open(en_stopword_path,\"r\")   \n",
    "stop_sents_en = f.read()    \n",
    "f.close()   \n",
    "stop_sents_en = stop_sents_en.split('\\n')\n",
    "f = open(common_stopword_path,\"r\")    \n",
    "stop_sents_common = f.read()   \n",
    "f.close()  \n",
    "stop_sents_common = stop_sents_common.split('\\n')\n",
    "stop_words = list(set(stop_sents_cn+stop_sents_common+stop_sents_en))\n",
    "print('停止词数量：',len(stop_words))\n",
    "\n",
    "print('2.2 加载保留词')\n",
    "jieba.load_userdict(keepword_path)   \n",
    "\n",
    "print('2.2~ 加载同义词')\n",
    "combine_dict = {}\n",
    "for line in open(synonyms_path, \"r\"):\n",
    "    seperate_word = line.strip().split(\" \")\n",
    "    num = len(seperate_word)\n",
    "    for i in range(1, num):\n",
    "        combine_dict[seperate_word[i]] = seperate_word[0]\n",
    "print('同义词对数量：', len(combine_dict))\n",
    "\n",
    "def clean_then_tokenize(x):\n",
    "    x = str(x)\n",
    "    x = emoji.replace_emoji(x, replace='')     # 去除emoji\n",
    "    x = convert(x, 'zh-cn')                    # 繁简转换\n",
    "    x = re.sub(u\"\\\\(.*?\\\\)|\\\\{.*?\\\\}|\\\\[.*?\\\\]|\\\\<.*?\\\\>\", \"\", x)  # 去除括号及其内部\n",
    "\n",
    "    ## 分词，去停止词，同时pos筛选\n",
    "    result = []\n",
    "    words = pseg.cut(x)\n",
    "    for word, flag in words:\n",
    "        cn_word = re.findall('[\\u4e00-\\u9fa5]', word) # 只保留汉字\n",
    "        cn_word = ''.join(cn_word) \n",
    "        if cn_word!='':\n",
    "            word=cn_word\n",
    "        word = word.lower()\n",
    "        if (flag in legal_pos) and (word not in stop_words):\n",
    "            result.append(word.rstrip().lstrip())\n",
    "    return list(filter(lambda x : x != '', result))\n",
    "\n",
    "def replace_synonyms(x):\n",
    "    result = []\n",
    "    for word in x:\n",
    "        if word in combine_dict:\n",
    "            result.append(combine_dict[word])\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "print('2.2 开始分词')\n",
    "df['tokens_list'] = df['body'].apply(lambda x: clean_then_tokenize(x))\n",
    "df['tokens_list'] = df['tokens_list'].apply(lambda x: replace_synonyms(x))\n",
    "df['tokens'] = df['tokens_list'].apply(lambda x: ' '.join(x))\n",
    "df = df.drop(df[df['tokens']==''].index)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1,inplace=True)\n",
    "print('当前数据量', df.shape)\n",
    "\n",
    "print('保存分词后的数据')\n",
    "df.insert(0, 'org_date_start', datetime.date.today()+timedelta(days=-8))\n",
    "df.insert(0, 'org_date_end', datetime.date.today()+timedelta(days=-2))\n",
    "df.to_csv(tokenized_filename, header=False, encoding='utf-8-sig', index=False, sep='\\t')\n",
    "\n",
    "print('3. TF-IDF建模：')\n",
    "df['idx'] = df.index\n",
    "tfidf_model = TfidfVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",  # enable 1-char words\n",
    "    lowercase=False,\n",
    "    max_df=tfidf_max_df, \n",
    "    max_features=tfidf_max_features  # only depends on tf\n",
    ")\n",
    "\n",
    "weight = tfidf_model.fit_transform(df['tokens']).toarray()\n",
    "word = tfidf_model.get_feature_names()\n",
    "df_word_weight = pd.DataFrame(data=weight, columns=word)\n",
    "topwords = df_word_weight.sum(axis=0)\n",
    "print('TFIDF后的数据大小', df_word_weight.shape)\n",
    "print('Topwords:')\n",
    "print(topwords.index.tolist())\n",
    "\n",
    "print('3.1 提取每个session的关键词：')\n",
    "def get_keywords(x, topK=tfidf_top_k):\n",
    "    key_words = []\n",
    "    for i, w in enumerate(weight[x]):\n",
    "        if w>0 :\n",
    "            key_words.append( (word[i],float('{:.3f}'.format(w))) )\n",
    "\n",
    "    key_words.sort(key=lambda y: y[1], reverse=True)     \n",
    "    result = key_words[:topK]\n",
    "    return [x for x,_ in result]\n",
    "\n",
    "df['keywords'] = df['idx'].apply(lambda x: get_keywords(x))\n",
    "df.drop('idx', inplace=True, axis=1)\n",
    "\n",
    "t2=time()\n",
    "print('预处理耗时：', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def find_stardard_Q(keys, Qs):\n",
    "    keys = keys.split('+')\n",
    "    for Q in Qs:\n",
    "        if all(key in Q for key in keys):\n",
    "            return Q\n",
    "    return np.nan\n",
    "\n",
    "def add_stardard_Q(df, standard_Q_path):\n",
    "    standard_Q = pd.read_excel(standard_Q_path,engine='openpyxl')\n",
    "    Qs = standard_Q['业务Q'].tolist()\n",
    "    df['standard_Q'] = df['gram'].apply(lambda x: find_stardard_Q(x, Qs))\n",
    "    print('新挖掘问题数量',len(df))\n",
    "    print('不在标准Q里的数量',df['standard_Q'].isna().sum())\n",
    "    print('占比',df['standard_Q'].isna().sum()/len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_bigrams_report(df, topwords, result_df_path, standard_Q_path, time_limit=600,\n",
    "                       show_samples=10, grams_count=100, occur=0.0001, window=2):\n",
    "    \n",
    "    t1=time()\n",
    "    \n",
    "    # input for calculating bigram pmi scores\n",
    "    data_pmi_input = []\n",
    "    for words in df['tokens_list'].tolist():\n",
    "        data_pmi_input.extend(words)\n",
    "        for i in range(window-1):\n",
    "            data_pmi_input.extend('|')\n",
    "    \n",
    "    # calculate bigram pmi scores\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    bi_finder = BigramCollocationFinder.from_words(data_pmi_input,window_size=window)  # default window size = 2 \n",
    "    \n",
    "    # save bigram pmi scores and generate report\n",
    "    bigrams = []\n",
    "    result_df = pd.DataFrame()\n",
    "    if 0<occur<1:\n",
    "        occur = len(df)*occur\n",
    " \n",
    "    for w1w2, score in bi_finder.score_ngrams(bigram_measures.pmi): # iterate grams and its score\n",
    "        \n",
    "        t2=time()\n",
    "        if t2-t1>time_limit or grams_count==0:\n",
    "            break\n",
    "            \n",
    "        w1, w2 = w1w2\n",
    "        list_w1w2 = sorted(set([w1, w2]))\n",
    "        \n",
    "        if  (w1!=w2) and (w1 in topwords) and (w2 in topwords) and len(list_w1w2)==2 and (list_w1w2 not in bigrams):\n",
    "            bigrams.append(list_w1w2)\n",
    "            temp_df = df['keywords'].apply(set(list_w1w2).issubset)\n",
    "            cnt = temp_df.sum()\n",
    "            row_result = dict()\n",
    "            \n",
    "            if cnt>occur:\n",
    "                print(list_w1w2,grams_count)\n",
    "                row_result['gram']=w1+'+'+w2\n",
    "                row_result['pmi']=score\n",
    "                row_result['occurance']=cnt\n",
    "                sample_df = df.iloc[temp_df[temp_df==True].index][:show_samples]\n",
    "                sample_df = sample_df[['session_id','body']]\n",
    "                row_result['samples'] = dict(zip(sample_df.session_id, sample_df.body))\n",
    "                result_df = result_df.append(row_result, ignore_index=True)\n",
    "                grams_count-=1\n",
    "        \n",
    "    \n",
    "    result_df = add_stardard_Q(result_df, standard_Q_path)\n",
    "    result_df = result_df[['gram', 'occurance', 'pmi', 'standard_Q', 'samples']]\n",
    "    result_df.insert(0, 'org_date_start', datetime.date.today()+timedelta(days=-8))\n",
    "    result_df.insert(0, 'org_date_end', datetime.date.today()+timedelta(days=-2))\n",
    "    result_df.to_csv(result_df_path, header=False,index=False,encoding='utf-8-sig', sep='\\t')\n",
    "    print(len(result_df), ' grams')\n",
    "    return result_df\n",
    "\n",
    "print('4.1 提取Bigrams：')\n",
    "\n",
    "result_bigram = get_bigrams_report(\n",
    "    df, \n",
    "    topwords, \n",
    "    grams_count=bigrams_count, \n",
    "    occur=bigrams_occur, \n",
    "    window=bigrams_window,\n",
    "    show_samples=bigrams_show_samples, \n",
    "    time_limit=bigrams_time_limit,\n",
    "    result_df_path=bigrams_result_df_path, \n",
    "    standard_Q_path=standard_Q_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_trigrams_report(df, bigrams, result_df_path, standard_Q_path, time_limit=1000,\n",
    "                        grams_count=200, show_samples=5, occur=0.0001, window=5):\n",
    "\n",
    "    t1=time()\n",
    "    \n",
    "    # input for calculating trigram pmi scores\n",
    "    data_pmi_input = []\n",
    "    for words in df['tokens_list'].tolist():\n",
    "        data_pmi_input.extend(words)\n",
    "        for i in range(window-1):\n",
    "            data_pmi_input.extend('|')\n",
    "    \n",
    "    # calculate trigram pmi scores\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    tri_finder = TrigramCollocationFinder.from_words(data_pmi_input,window_size=window) \n",
    "    \n",
    "    # save trigram pmi scores # generate report\n",
    "    trigrams = []\n",
    "    result_df = pd.DataFrame()\n",
    "    if 0<occur<1:\n",
    "        occur = occur*len(df)\n",
    "    for w1w2w3, score in tri_finder.score_ngrams(trigram_measures.pmi):\n",
    "        t2=time()\n",
    "        if grams_count==0 or t2-t1>time_limit:\n",
    "            break\n",
    "        is_trigram_recorded = False\n",
    "        w1, w2, w3 = w1w2w3\n",
    "        set_w1w2w3 = set([w1,w2,w3])\n",
    "        list_w1w2w3 = sorted(set_w1w2w3)\n",
    "        \n",
    "        for set_w1w2 in bigrams:\n",
    "            if set_w1w2.issubset(set_w1w2w3) and len(set_w1w2w3)==3 and (list_w1w2w3 not in trigrams):\n",
    "                trigrams.append(list_w1w2w3)\n",
    "                is_trigram_recorded=True\n",
    "                break\n",
    "            \n",
    "        if is_trigram_recorded:\n",
    "            temp_df = df['keywords'].apply(set_w1w2w3.issubset)\n",
    "            cnt = temp_df.sum()\n",
    "            row_result = dict()\n",
    "            if cnt>occur:\n",
    "                print(list_w1w2w3, grams_count)\n",
    "                row_result['gram']=w1+'+'+w2+'+'+w3\n",
    "                row_result['pmi']=score\n",
    "                row_result['occurance']=cnt\n",
    "                sample_df = df.iloc[temp_df[temp_df==True].index][:show_samples]\n",
    "                sample_df = sample_df[['session_id','body']]\n",
    "                row_result['samples'] = dict(zip(sample_df.session_id, sample_df.body))\n",
    "                result_df = result_df.append(row_result, ignore_index=True)\n",
    "                grams_count-=1\n",
    "\n",
    "    result_df = add_stardard_Q(result_df, standard_Q_path)\n",
    "    result_df = result_df[['gram', 'occurance', 'pmi', 'standard_Q', 'samples']]\n",
    "    result_df.insert(0, 'org_date_start', datetime.date.today()+timedelta(days=-8))\n",
    "    result_df.insert(0, 'org_date_end', datetime.date.today()+timedelta(days=-2))\n",
    "    result_df.to_csv(result_df_path, header=False, index=False, encoding='utf-8-sig', sep='\\t')\n",
    "    print(len(result_df), ' grams')\n",
    "    return result_df\n",
    "\n",
    "bigrams = result_bigram.gram.tolist()\n",
    "bigrams = [set(x.split('+')) for x in bigrams]\n",
    "\n",
    "print('4.2 提取Trigrams：')\n",
    "result_trigram=get_trigrams_report(\n",
    "    df, bigrams, \n",
    "    result_df_path=trigrams_result_df_path,\n",
    "    standard_Q_path=standard_Q_path, \n",
    "    time_limit=trigrams_time_limit,\n",
    "    grams_count=trigrams_count, \n",
    "    show_samples=trigrams_show_samples, \n",
    "    occur=trigrams_occur, \n",
    "    window=trigrams_window\n",
    ")\n",
    "print('全部处理完毕！')\n",
    "print('session数量',df.shape)\n",
    "print('bigram数量', result_bigram.shape)\n",
    "print('trigram数量',result_trigram.shape)\n",
    "\n",
    "print('保存配置文件：')\n",
    "copyfile('config.py', config_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "print('数据存入临时表：')\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    " LOAD DATA LOCAL INPATH '{}' OVERWRITE INTO TABLE {};\" \"\"\".format(tokenized_filename, tokenized_tablename)\n",
    ")\n",
    "os.system(\n",
    " \"\"\"hive -e \" \n",
    " LOAD DATA LOCAL INPATH '{}' OVERWRITE INTO TABLE {};\" \"\"\".format(bigrams_result_df_path, bigram_tablename)\n",
    ")\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    " LOAD DATA LOCAL INPATH '{}' OVERWRITE INTO TABLE {};\" \"\"\".format(trigrams_result_df_path, trigram_tablename)\n",
    ")\n",
    "\n",
    "\n",
    "today = date.today()\n",
    "d = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print('临时表数据写入正式表：')\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    "    insert into table {} partition(d='{}')\n",
    "    select * from {};\n",
    " \" \"\"\".format(adm_bigram_tablename, d, bigram_tablename)\n",
    ")\n",
    "\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    "    insert into table {} partition(d='{}')\n",
    "    select * from {};\n",
    " \" \"\"\".format(adm_trigram_tablename, d, trigram_tablename)\n",
    ")\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    "    insert into table {} partition(d='{}')\n",
    "    select * from {};\n",
    " \" \"\"\".format(adm_tokenized_tablename, d, tokenized_tablename)\n",
    ")\n",
    "\n",
    "\n",
    "t2 = time()\n",
    "print('结果写入数据库耗时', t2-t1)\n",
    "\n",
    "print('脚本执行完毕！')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ssssssssssssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 16542: Expected 6 fields in line 16542, saw 7\n",
      "Skipping line 34645: Expected 6 fields in line 34645, saw 15\n",
      "Skipping line 54071: Expected 6 fields in line 54071, saw 8\n",
      "Skipping line 70152: Expected 6 fields in line 70152, saw 9\n",
      "Skipping line 91774: Expected 6 fields in line 91774, saw 7\n",
      "Skipping line 92093: Expected 6 fields in line 92093, saw 7\n",
      "Skipping line 100187: Expected 6 fields in line 100187, saw 9\n",
      "Skipping line 100188: Expected 6 fields in line 100188, saw 9\n",
      "Skipping line 104560: Expected 6 fields in line 104560, saw 8\n",
      "Skipping line 108757: Expected 6 fields in line 108757, saw 9\n",
      "Skipping line 135421: Expected 6 fields in line 135421, saw 8\n",
      "Skipping line 159649: Expected 6 fields in line 159649, saw 9\n",
      "Skipping line 213990: Expected 6 fields in line 213990, saw 7\n",
      "Skipping line 214006: Expected 6 fields in line 214006, saw 7\n",
      "Skipping line 214644: Expected 6 fields in line 214644, saw 12\n",
      "Skipping line 258717: Expected 6 fields in line 258717, saw 7\n",
      "Skipping line 259041: Expected 6 fields in line 259041, saw 16\n",
      "Skipping line 269515: Expected 6 fields in line 269515, saw 9\n",
      "Skipping line 292911: Expected 6 fields in line 292911, saw 7\n",
      "Skipping line 293347: Expected 6 fields in line 293347, saw 18\n",
      "Skipping line 311675: Expected 6 fields in line 311675, saw 9\n",
      "Skipping line 318116: Expected 6 fields in line 318116, saw 7\n",
      "Skipping line 331808: Expected 6 fields in line 331808, saw 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集shape:  (399868, 6)\n",
      "1. 清洗和筛选\n",
      "对session_id进行计数编码\n",
      "总session数量 33678\n",
      "1.1 round完整性筛选\n",
      "有rank1的session数量： (33442,)\n",
      "有rank1的session比例： 0.9929924579844409\n",
      "rank=round的session数量 (33459,)\n",
      "rank=round的session比例 0.9934972385533583\n",
      "完整的session数量： (33431, 1)\n",
      "完整的session占比： 0.9926658352633767\n",
      "1.2 根据round内部发言筛选\n",
      "聚合type字段\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/powerop/work/conda/envs/IM_high_freq/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "确保session中同时有顾客客服和机器人的发言\n",
      "此时的session数量 (24917, 3)\n",
      "取最后的顾客发言之前的一个机器发言\n",
      "此时数据集shape:  (231610, 10)\n",
      "1.3 根据type和msgtype筛选\n",
      "此时数据集shape:  (90297, 10)\n",
      "1.4 JSON 转文本\n",
      "1.5 Session聚合\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此时数据集shape:  (24849, 2)\n",
      "删除转义符，防止保存csv的时候出错\n",
      "2. 分词\n",
      "2.1 读取停止词\n",
      "停止词数量： 903\n",
      "2.2 加载保留词\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.980 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2~ 加载同义词\n",
      "同义词对数量： 20\n",
      "2.2 开始分词\n",
      "当前数据量 (22184, 4)\n",
      "保存分词后的数据\n"
     ]
    }
   ],
   "source": [
    "t1=time()\n",
    "\n",
    "df=pd.read_csv(input_filename, header=0,sep='\\t',error_bad_lines=False,engine='python')\n",
    "df = df[['session_id','datachange_createtime',  'msgtype', 'type', 'rank', 'body']]\n",
    "print('原始数据集shape: ', df.shape)\n",
    "\n",
    "print('1. 清洗和筛选')\n",
    "\n",
    "print('对session_id进行计数编码')\n",
    "df_session = df['session_id']\n",
    "ce = CountEncoder(cols=['session_id'])\n",
    "df_encoded = ce.fit_transform(df)\n",
    "df_encoded.columns = ['round','datachange_createtime',  'msgtype', 'type', 'rank', 'body']\n",
    "df = pd.concat([df_session, df_encoded], axis=1)\n",
    "\n",
    "session_ids = df['session_id'].drop_duplicates()\n",
    "session_ids_count = session_ids.shape[0]\n",
    "print('总session数量', session_ids_count)\n",
    "\n",
    "print('1.1 round完整性筛选')\n",
    "rank1_session_ids = df[df['rank']==1]['session_id'].drop_duplicates()\n",
    "print('有rank1的session数量：', rank1_session_ids.shape)\n",
    "print('有rank1的session比例：', rank1_session_ids.shape[0] / session_ids.shape[0])   \n",
    "\n",
    "rank_eq_round_session_ids = df[df['rank']==df['round']]['session_id'].drop_duplicates()\n",
    "print('rank=round的session数量', rank_eq_round_session_ids.shape)\n",
    "print('rank=round的session比例', rank_eq_round_session_ids.shape[0] / session_ids.shape[0] )       \n",
    "\n",
    "session_ids = pd.merge(rank1_session_ids, session_ids, how='inner', on='session_id')\n",
    "session_ids = pd.merge(rank_eq_round_session_ids, session_ids, how='inner', on='session_id')\n",
    "print('完整的session数量：', session_ids.shape)\n",
    "print('完整的session占比：', session_ids.shape[0] / session_ids_count)  # session 既完整也能进人工 比例\n",
    "\n",
    "df = pd.merge(session_ids, df, how='left', on='session_id')\n",
    "\n",
    "print('1.2 根据round内部发言筛选')\n",
    "print('聚合type字段')\n",
    "df_temp = df[['session_id', 'type']]\n",
    "df_temp['type'] = df_temp['type'].apply(lambda x: str(x))\n",
    "df_temp = df_temp.groupby(by='session_id')['type'].sum().to_frame()  # 聚合type\n",
    "df_temp = df_temp.reset_index()\n",
    "df_temp.columns = ['session_id', 'type_seq']\n",
    "\n",
    "print('确保session中同时有顾客客服和机器人的发言')\n",
    "def find_man_and_machine(x):  \n",
    "    if ('1' in x and '2' in x and '3' in x) or ('1' in x and '2' in x and '4' in x):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_temp['man_and_machine'] = df_temp['type_seq'].apply(lambda x: find_man_and_machine(x))\n",
    "df_temp = df_temp[df_temp['man_and_machine']==True]\n",
    "print('此时的session数量', df_temp.shape)\n",
    "\n",
    "\n",
    "print('取最后的顾客发言之前的一个机器发言')\n",
    "def find_last_machine(x):\n",
    "    last1idx = len(x) - x[::-1].index('1') - 1\n",
    "    x = x[:last1idx]\n",
    "    if '2' in x:\n",
    "        return len(x) - x[::-1].index('2') \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_temp['last_machine'] = df_temp['type_seq'].apply(lambda x: find_last_machine(x))\n",
    "df = pd.merge(df_temp, df, how='left', on='session_id')\n",
    "df = df[df['rank'] > df['last_machine']]\n",
    "print('此时数据集shape: ', df.shape)\n",
    "\n",
    "print('1.3 根据type和msgtype筛选')\n",
    "type_dict = {1:'客人',2:'机器',3:\"酒店客服人员\",4:'携程客服人员', 5:'服务评价'}\n",
    "msgtype_dict = {0:'手打文本',1:'图像',2:'酒店或房型',3:'视频',4:'语音',5:'文件或链接',6:'坐标',7:'json模板'} \n",
    "df = df[(df['type']==1) ]\n",
    "df = df[(df['msgtype']==0) | (df['msgtype']==7)]\n",
    "print('此时数据集shape: ', df.shape)\n",
    "\n",
    "print('1.4 JSON 转文本')\n",
    "def get_title(body):\n",
    "    if 'title' in body and '{' in body and '}' in body:\n",
    "        return json.loads(body, strict=False)['title']\n",
    "    else:\n",
    "        return body\n",
    "df['body']=df['body'].apply(lambda x : get_title(x))\n",
    "\n",
    "print('1.5 Session聚合')\n",
    "df = df[['session_id','rank','body']]\n",
    "df = df.groupby(by='session_id').body.sum().to_frame()  # 聚合body\n",
    "df = df.reset_index()\n",
    "print('此时数据集shape: ', df.shape)\n",
    "\n",
    "print('删除转义符，防止保存csv的时候出错')\n",
    "df['body'] = df['body'].apply(lambda x: x.replace('\\r',''))  \n",
    "df['body'] = df['body'].apply(lambda x: x.replace('\\n',''))  \n",
    "df['body'] = df['body'].apply(lambda x: x.replace('\\t',''))  \n",
    "\n",
    "print('2. 分词')\n",
    "print('2.1 读取停止词')\n",
    "f = open(cn_stopword_path,\"r\")   \n",
    "stop_sents_cn = f.read()    \n",
    "f.close()   \n",
    "stop_sents_cn = stop_sents_cn.split('\\n')\n",
    "f = open(en_stopword_path,\"r\")   \n",
    "stop_sents_en = f.read()    \n",
    "f.close()   \n",
    "stop_sents_en = stop_sents_en.split('\\n')\n",
    "f = open(common_stopword_path,\"r\")    \n",
    "stop_sents_common = f.read()   \n",
    "f.close()  \n",
    "stop_sents_common = stop_sents_common.split('\\n')\n",
    "stop_words = list(set(stop_sents_cn+stop_sents_common+stop_sents_en))\n",
    "print('停止词数量：',len(stop_words))\n",
    "\n",
    "print('2.2 加载保留词')\n",
    "jieba.load_userdict(keepword_path)   \n",
    "\n",
    "print('2.2~ 加载同义词')\n",
    "combine_dict = {}\n",
    "for line in open(synonyms_path, \"r\"):\n",
    "    seperate_word = line.strip().split(\" \")\n",
    "    num = len(seperate_word)\n",
    "    for i in range(1, num):\n",
    "        combine_dict[seperate_word[i]] = seperate_word[0]\n",
    "print('同义词对数量：', len(combine_dict))\n",
    "\n",
    "def clean_then_tokenize(x):\n",
    "    x = str(x)\n",
    "    x = emoji.replace_emoji(x, replace='')     # 去除emoji\n",
    "    x = convert(x, 'zh-cn')                    # 繁简转换\n",
    "    x = re.sub(u\"\\\\(.*?\\\\)|\\\\{.*?\\\\}|\\\\[.*?\\\\]|\\\\<.*?\\\\>\", \"\", x)  # 去除括号及其内部\n",
    "\n",
    "    ## 分词，去停止词，同时pos筛选\n",
    "    result = []\n",
    "    words = pseg.cut(x)\n",
    "    for word, flag in words:\n",
    "        cn_word = re.findall('[\\u4e00-\\u9fa5]', word) # 只保留汉字\n",
    "        cn_word = ''.join(cn_word) \n",
    "        if cn_word!='':\n",
    "            word=cn_word\n",
    "        word = word.lower()\n",
    "        if (flag in legal_pos) and (word not in stop_words):\n",
    "            result.append(word.rstrip().lstrip())\n",
    "    return list(filter(lambda x : x != '', result))\n",
    "\n",
    "def replace_synonyms(x):\n",
    "    result = []\n",
    "    for word in x:\n",
    "        if word in combine_dict:\n",
    "            result.append(combine_dict[word])\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "print('2.2 开始分词')\n",
    "df['tokens_list'] = df['body'].apply(lambda x: clean_then_tokenize(x))\n",
    "df['tokens_list'] = df['tokens_list'].apply(lambda x: replace_synonyms(x))\n",
    "df['tokens'] = df['tokens_list'].apply(lambda x: ' '.join(x))\n",
    "df = df.drop(df[df['tokens']==''].index)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1,inplace=True)\n",
    "print('当前数据量', df.shape)\n",
    "\n",
    "print('保存分词后的数据')\n",
    "df.insert(0, 'org_date_start', datetime.date.today()+timedelta(days=-8))\n",
    "df.insert(0, 'org_date_end', datetime.date.today()+timedelta(days=-2))\n",
    "df.to_csv(tokenized_filename, header=False, encoding='utf-8-sig', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_date_end</th>\n",
       "      <th>org_date_start</th>\n",
       "      <th>session_id</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_list</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>100000091497334</td>\n",
       "      <td>入住要24小时核酸检测吗要24小时核酸检测吗外地的24小时不行吗？本地的码没那么快出来</td>\n",
       "      <td>[入住, 小时, 核酸检测, 小时, 核酸检测, 外地, 小时, 的码, 没, 快]</td>\n",
       "      <td>入住 小时 核酸检测 小时 核酸检测 外地 小时 的码 没 快</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>300000090797638</td>\n",
       "      <td>你好我这会下单那个房间没有了吗人工</td>\n",
       "      <td>[下单, 房间]</td>\n",
       "      <td>下单 房间</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14882</th>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>600000090997161</td>\n",
       "      <td>位置不是疫情管控的位置吧只要出示绿码和行程码九可以了对吧昨天做的核酸还没有出结果我和我女朋友...</td>\n",
       "      <td>[位置, 疫情, 管控, 位置, 出示, 绿码, 行程码, 做, 核酸, 出, 女朋友, 核...</td>\n",
       "      <td>位置 疫情 管控 位置 出示 绿码 行程码 做 核酸 出 女朋友 核酸 没出 有毒 只能 退...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10056</th>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>400000090650513</td>\n",
       "      <td>我要取消订单未入住，行程有变店家客服电话？</td>\n",
       "      <td>[取消, 订单, 未, 入住, 行程, 有变, 店家, 电话]</td>\n",
       "      <td>取消 订单 未 入住 行程 有变 店家 电话</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>200000091256568</td>\n",
       "      <td>费用</td>\n",
       "      <td>[费用]</td>\n",
       "      <td>费用</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      org_date_end org_date_start       session_id  \\\n",
       "2594    2022-04-17     2022-04-11  100000091497334   \n",
       "5947    2022-04-17     2022-04-11  300000090797638   \n",
       "14882   2022-04-17     2022-04-11  600000090997161   \n",
       "10056   2022-04-17     2022-04-11  400000090650513   \n",
       "4194    2022-04-17     2022-04-11  200000091256568   \n",
       "\n",
       "                                                    body  \\\n",
       "2594         入住要24小时核酸检测吗要24小时核酸检测吗外地的24小时不行吗？本地的码没那么快出来   \n",
       "5947                                   你好我这会下单那个房间没有了吗人工   \n",
       "14882  位置不是疫情管控的位置吧只要出示绿码和行程码九可以了对吧昨天做的核酸还没有出结果我和我女朋友...   \n",
       "10056                              我要取消订单未入住，行程有变店家客服电话？   \n",
       "4194                                                  费用   \n",
       "\n",
       "                                             tokens_list  \\\n",
       "2594          [入住, 小时, 核酸检测, 小时, 核酸检测, 外地, 小时, 的码, 没, 快]   \n",
       "5947                                            [下单, 房间]   \n",
       "14882  [位置, 疫情, 管控, 位置, 出示, 绿码, 行程码, 做, 核酸, 出, 女朋友, 核...   \n",
       "10056                    [取消, 订单, 未, 入住, 行程, 有变, 店家, 电话]   \n",
       "4194                                                [费用]   \n",
       "\n",
       "                                                  tokens  \n",
       "2594                     入住 小时 核酸检测 小时 核酸检测 外地 小时 的码 没 快  \n",
       "5947                                               下单 房间  \n",
       "14882  位置 疫情 管控 位置 出示 绿码 行程码 做 核酸 出 女朋友 核酸 没出 有毒 只能 退...  \n",
       "10056                             取消 订单 未 入住 行程 有变 店家 电话  \n",
       "4194                                                  费用  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. TF-IDF建模：\n",
      "TFIDF后的数据大小 (30820, 1000)\n",
      "3.1 提取每个session的关键词：\n",
      "130.4496250152588\n"
     ]
    }
   ],
   "source": [
    "print('3. TF-IDF建模：')\n",
    "df['idx'] = df.index\n",
    "tfidf_model = TfidfVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",  # enable 1-char words\n",
    "    lowercase=False,\n",
    "    max_df=tfidf_max_df, \n",
    "    max_features=tfidf_max_features  # only depends on tf\n",
    ")\n",
    "\n",
    "weight = tfidf_model.fit_transform(df['tokens']).toarray()\n",
    "word = tfidf_model.get_feature_names()\n",
    "df_word_weight = pd.DataFrame(data=weight, columns=word)\n",
    "topwords = df_word_weight.sum(axis=0)\n",
    "print('TFIDF后的数据大小', df_word_weight.shape)\n",
    "print('Topwords:')\n",
    "print(topwords.index.tolist())\n",
    "\n",
    "print('3.1 提取每个session的关键词：')\n",
    "def get_keywords(x, topK=tfidf_top_k):\n",
    "    key_words = []\n",
    "    for i, w in enumerate(weight[x]):\n",
    "        if w>0 :\n",
    "            key_words.append( (word[i],float('{:.3f}'.format(w))) )\n",
    "\n",
    "    key_words.sort(key=lambda y: y[1], reverse=True)     \n",
    "    result = key_words[:topK]\n",
    "    return [x for x,_ in result]\n",
    "\n",
    "df['keywords'] = df['idx'].apply(lambda x: get_keywords(x))\n",
    "df.drop('idx', inplace=True, axis=1)\n",
    "\n",
    "t2=time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1 提取Bigrams：\n",
      "['洗漱', '用品'] 200\n",
      "['fi', 'wi'] 199\n",
      "['电', '竞'] 198\n",
      "['插', '网线'] 197\n",
      "['厅', '室'] 196\n",
      "['效果', '隔音'] 195\n",
      "['学院', '科技'] 194\n",
      "['装修', '风格'] 193\n",
      "['电梯', '要刷'] 192\n",
      "['主题', '浪漫'] 191\n",
      "['行政', '酒廊'] 190\n",
      "['感谢您', '非常'] 189\n",
      "['有限公司', '税号'] 188\n",
      "['一次性', '拖鞋'] 187\n",
      "['差价', '补'] 186\n",
      "['征用', '政府'] 185\n",
      "['专票', '能开'] 184\n",
      "['开具', '水单'] 183\n",
      "['短信', '给我发'] 182\n",
      "['房卡', '要刷'] 181\n",
      "['娱乐', '项目'] 180\n",
      "['稳定', '网络'] 179\n",
      "['寄存', '行李'] 178\n",
      "['支行', '账号'] 177\n",
      "['qq', '邮箱'] 176\n",
      "['手续费', '扣'] 175\n",
      "['娱乐', '设施'] 174\n",
      "['健康', '监测'] 173\n",
      "['智能', '马桶'] 172\n",
      "['厨具', '电磁炉'] 171\n",
      "['有限公司', '科技'] 170\n",
      "['午餐', '晚餐'] 169\n",
      "['人间', '男生'] 168\n",
      "['拖鞋', '用品'] 167\n",
      "['病例', '确诊'] 166\n",
      "['报', '身份证号'] 165\n",
      "['收取', '违约金'] 164\n",
      "['宠物', '携带'] 163\n",
      "['a', 'b'] 162\n",
      "['公司', '名称'] 161\n",
      "['团购', '抖音'] 160\n",
      "['变', '黄码'] 159\n",
      "['新增', '病例'] 158\n",
      "['厨具', '锅'] 157\n",
      "['电脑', '配置'] 156\n",
      "['厨房', '锅'] 155\n",
      "['密码', '无线'] 154\n",
      "['下雨', '天气'] 153\n",
      "['拖鞋', '牙刷'] 152\n",
      "['一次性', '马桶'] 151\n",
      "['坐', '高铁'] 150\n",
      "['听', '得到'] 149\n",
      "['持有', '未成年'] 148\n",
      "['有变', '行程'] 147\n",
      "['fi', '密码'] 146\n",
      "['卫生间', '独立'] 145\n",
      "['一楼', '二楼'] 144\n",
      "['洗', '洗衣服'] 143\n",
      "['变', '黄'] 142\n",
      "['大陆', '居民'] 141\n",
      "['升房', '权益'] 140\n",
      "['健康', '弹窗'] 139\n",
      "['一次性', '牙刷'] 138\n",
      "['wifi', '密码'] 137\n",
      "['全额', '退款'] 136\n",
      "['刷卡', '电梯'] 135\n",
      "['单独', '未满'] 134\n",
      "['乐园', '水'] 133\n",
      "['投影仪', '电影'] 132\n",
      "['投屏', '电视'] 131\n",
      "['自动', '退回'] 130\n",
      "['收取', '费用'] 129\n",
      "['电磁炉', '锅'] 128\n",
      "['学校', '封校'] 127\n",
      "['休息', '早点'] 126\n",
      "['携带', '狗狗'] 125\n",
      "['名字', '报'] 124\n",
      "['女生', '男生'] 123\n",
      "['没收', '短信'] 122\n",
      "['厕所', '马桶'] 121\n",
      "['中心', '游客'] 120\n",
      "['不好', '信号'] 119\n",
      "['坐', '大巴'] 118\n",
      "['有变', '计划'] 117\n",
      "['窗户', '走廊'] 116\n",
      "['抬头', '税号'] 115\n",
      "['控区', '管控'] 114\n",
      "['vx', '微'] 113\n",
      "['押金', '没退'] 112\n",
      "['变成', '黄码'] 111\n",
      "['扣', '违约金'] 110\n",
      "['账', '退款'] 109\n",
      "['居家', '支持'] 108\n",
      "['大人', '小孩'] 107\n",
      "['外国人', '接受'] 106\n",
      "['好不好', '隔音'] 105\n",
      "['受', '影响'] 104\n",
      "['支行', '有限公司'] 103\n",
      "['恒温', '泳池'] 102\n",
      "['恒温', '游泳池'] 101\n",
      "['网线', '网速'] 100\n",
      "['实际', '未'] 99\n",
      "['泡', '私汤'] 98\n",
      "['买票', '单独'] 97\n",
      "['不远', '远'] 96\n",
      "['同意', '家长'] 95\n",
      "['寄', '快递'] 94\n",
      "['泡池', '私人'] 93\n",
      "['恒温', '游泳'] 92\n",
      "['电子', '证件'] 91\n",
      "['含', '早'] 90\n",
      "['允许', '携带'] 89\n",
      "['外省', '自驾'] 88\n",
      "['填', '姓名'] 87\n",
      "['楼层', '高'] 86\n",
      "['外出', '本地人'] 85\n",
      "['几楼', '大堂'] 84\n",
      "['充电器', '忘'] 83\n",
      "['亲子', '套房'] 82\n",
      "['充电', '线'] 81\n",
      "['游泳', '游泳池'] 80\n",
      "['晚餐', '自助餐'] 79\n",
      "['人间', '女生'] 78\n",
      "['晚餐', '自助'] 77\n",
      "['做饭', '厨房'] 76\n",
      "['交', '押金'] 75\n",
      "['外卖', '送到'] 74\n",
      "['做饭', '锅'] 73\n",
      "['情况', '特殊'] 72\n",
      "['最高', '楼层'] 71\n",
      "['接机', '送机'] 70\n",
      "['暖气', '空调'] 69\n",
      "['h', '持'] 68\n",
      "['一定', '机会'] 67\n",
      "['不错', '觉得'] 66\n",
      "['收到', '短信'] 65\n",
      "['洗澡', '热水'] 64\n",
      "['码', '绿'] 63\n",
      "['不上', '联系'] 62\n",
      "['放在', '桌子'] 61\n",
      "['居家', '监测'] 60\n",
      "['区', '风险'] 59\n",
      "['未满', '满'] 58\n",
      "['停', '车'] 57\n",
      "['床单', '换'] 56\n",
      "['下车', '站'] 55\n",
      "['恢复', '营业'] 54\n",
      "['天', '检'] 53\n",
      "['开票', '抬头'] 52\n",
      "['地区', '高风险'] 51\n",
      "['接到', '通知'] 50\n",
      "['健康码', '绿色'] 49\n",
      "['双', '绿码'] 48\n",
      "['冷', '暖气'] 47\n",
      "['发票', '开具'] 46\n",
      "['吃', '饭'] 45\n",
      "['做饭', '厨具'] 44\n",
      "['会员', '权益'] 43\n",
      "['可否', '告知'] 42\n",
      "['烧烤', '食材'] 41\n",
      "['中风险', '地区'] 40\n",
      "['低风险', '地区'] 39\n",
      "['海景房', '高层'] 38\n",
      "['服务', '洗衣'] 37\n",
      "['小院', '独立'] 36\n",
      "['入境', '接受'] 35\n",
      "['接站', '高铁站'] 34\n",
      "['处理', '赶紧'] 33\n",
      "['修改', '成'] 32\n",
      "['房卡', '放在'] 31\n",
      "['电竞房', '电脑'] 30\n",
      "['优惠', '长住'] 29\n",
      "['发票', '电子'] 28\n",
      "['会议室', '餐厅'] 27\n",
      "['乐园', '恒温'] 26\n",
      "['v', '加'] 25\n",
      "['专票', '开'] 24\n",
      "['充电器', '放在'] 23\n",
      "['租', '长期'] 22\n",
      "['商量一下', '朋友'] 21\n",
      "['小朋友', '适合'] 20\n",
      "['便宜', '更'] 19\n",
      "['隔离', '集中'] 18\n",
      "['收', '起来'] 17\n",
      "['加', '微'] 16\n",
      "['大人', '小朋友'] 15\n",
      "['双床房', '舒适'] 14\n",
      "['不错', '感觉'] 13\n",
      "['添加', '联系方式'] 12\n",
      "['快递', '收'] 11\n",
      "['手机', '投屏'] 10\n",
      "['信息', '开票'] 9\n",
      "['天', '接收'] 8\n",
      "['电子', '身份证'] 7\n",
      "['地区', '风险'] 6\n",
      "['出过', '未'] 5\n",
      "['支付', '线'] 4\n",
      "['独立', '院子'] 3\n",
      "['wx', '加'] 2\n",
      "['包含', '门票'] 1\n",
      "新挖掘问题数量 200\n",
      "不在标准Q里的数量 178\n",
      "占比 0.89\n",
      "200  grams\n"
     ]
    }
   ],
   "source": [
    "def find_stardard_Q(keys, Qs):\n",
    "    keys = keys.split('+')\n",
    "    for Q in Qs:\n",
    "        if all(key in Q for key in keys):\n",
    "            return Q\n",
    "    return np.nan\n",
    "\n",
    "def add_stardard_Q(df, standard_Q_path):\n",
    "    standard_Q = pd.read_excel(standard_Q_path,engine='openpyxl')\n",
    "    Qs = standard_Q['业务Q'].tolist()\n",
    "    df['standard_Q'] = df['gram'].apply(lambda x: find_stardard_Q(x, Qs))\n",
    "    print('新挖掘问题数量',len(df))\n",
    "    print('不在标准Q里的数量',df['standard_Q'].isna().sum())\n",
    "    print('占比',df['standard_Q'].isna().sum()/len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_bigrams_report(df, topwords, result_df_path, standard_Q_path, time_limit=600,\n",
    "                       show_samples=10, grams_count=100, occur=0.0001, window=2):\n",
    "    \n",
    "    t1=time()\n",
    "    \n",
    "    # input for calculating bigram pmi scores\n",
    "    data_pmi_input = []\n",
    "    for words in df['tokens_list'].tolist():\n",
    "        data_pmi_input.extend(words)\n",
    "        for i in range(window-1):\n",
    "            data_pmi_input.extend('|')\n",
    "    \n",
    "    # calculate bigram pmi scores\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    bi_finder = BigramCollocationFinder.from_words(data_pmi_input,window_size=window)  # default window size = 2 \n",
    "    \n",
    "    # save bigram pmi scores and generate report\n",
    "    bigrams = []\n",
    "    result_df = pd.DataFrame()\n",
    "    if 0<occur<1:\n",
    "        occur = len(df)*occur\n",
    " \n",
    "    for w1w2, score in bi_finder.score_ngrams(bigram_measures.pmi): # iterate grams and its score\n",
    "        \n",
    "        t2=time()\n",
    "        if t2-t1>time_limit or grams_count==0:\n",
    "            break\n",
    "            \n",
    "        w1, w2 = w1w2\n",
    "        list_w1w2 = sorted(set([w1, w2]))\n",
    "        \n",
    "        if  (w1!=w2) and (w1 in topwords) and (w2 in topwords) and len(list_w1w2)==2 and (list_w1w2 not in bigrams):\n",
    "            bigrams.append(list_w1w2)\n",
    "            temp_df = df['keywords'].apply(set(list_w1w2).issubset)\n",
    "            cnt = temp_df.sum()\n",
    "            row_result = dict()\n",
    "            \n",
    "            if cnt>occur:\n",
    "                print(list_w1w2,grams_count)\n",
    "                row_result['gram']=w1+'+'+w2\n",
    "                row_result['pmi']=score\n",
    "                row_result['occurance']=cnt\n",
    "                sample_df = df.iloc[temp_df[temp_df==True].index][:show_samples]\n",
    "                sample_df = sample_df[['session_id','body']]\n",
    "                row_result['samples'] = dict(zip(sample_df.session_id, sample_df.body))\n",
    "                result_df = result_df.append(row_result, ignore_index=True)\n",
    "                grams_count-=1\n",
    "        \n",
    "    \n",
    "    result_df = add_stardard_Q(result_df, standard_Q_path)\n",
    "    result_df = result_df[['gram', 'occurance', 'pmi', 'standard_Q', 'samples']]\n",
    "    result_df.insert(0, 'org_date_start', datetime.date.today()+timedelta(days=-8))\n",
    "    result_df.insert(0, 'org_date_end', datetime.date.today()+timedelta(days=-2))\n",
    "    result_df.to_csv(result_df_path, header=False,index=False,encoding='utf-8-sig', sep='\\t')\n",
    "    print(len(result_df), ' grams')\n",
    "    return result_df\n",
    "\n",
    "print('4.1 提取Bigrams：')\n",
    "\n",
    "\n",
    "\n",
    "result_bigram = get_bigrams_report(\n",
    "    df, \n",
    "    topwords, \n",
    "    grams_count=bigrams_count, \n",
    "    occur=bigrams_occur, \n",
    "    window=bigrams_window,\n",
    "    show_samples=bigrams_show_samples, \n",
    "    time_limit=bigrams_time_limit,\n",
    "    result_df_path=bigrams_result_df_path, \n",
    "    standard_Q_path=standard_Q_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 提取Trigrams：\n",
      "['房卡', '电梯', '要刷'] 100\n",
      "['fi', 'wi', '密码'] 99\n",
      "['qq', '有限公司', '税号'] 98\n",
      "['健康', '监测', '解除'] 97\n",
      "['天', '接收', '解除'] 96\n",
      "['居家', '接受', '监测'] 95\n",
      "['回', '居家', '监测'] 94\n",
      "['健康', '天', '监测'] 93\n",
      "['qq', '发票', '邮箱'] 92\n",
      "['楼层', '预留', '高'] 91\n",
      "['外省', '是否', '自驾'] 90\n",
      "['天', '居家', '监测'] 89\n",
      "['居家', '支持', '隔离'] 88\n",
      "['手机', '投屏', '电视'] 87\n",
      "['安排', '楼层', '高'] 86\n",
      "['办理', '电子', '证件'] 85\n",
      "['刷卡', '电梯', '需要'] 84\n",
      "['交', '店', '押金'] 83\n",
      "['发票', '开', '电子'] 82\n",
      "['天', '隔离', '集中'] 81\n",
      "['有变', '行程', '订单'] 80\n",
      "['取消', '有变', '行程'] 79\n",
      "['v', '加', '方便'] 78\n",
      "['有变', '行程', '退'] 77\n",
      "['刷卡', '电梯', '酒店'] 76\n",
      "['外卖', '房间', '送到'] 75\n",
      "['地区', '星号', '高风险'] 74\n",
      "['办理', '电子', '身份证'] 73\n",
      "['低风险', '地区', '行程卡'] 72\n",
      "['外省', '正常', '自驾'] 71\n",
      "['不上', '电话', '联系'] 70\n",
      "['低风险', '地区', '星号'] 69\n",
      "['低风险', '地区', '绿码'] 68\n",
      "['低风险', '地区', '带星'] 67\n",
      "['房间', '投屏', '电视'] 66\n",
      "['带', '电子', '身份证'] 65\n",
      "['wifi', '密码', '房间'] 64\n",
      "['低风险', '地区', '隔离'] 63\n",
      "['低风险', '地区', '小时'] 62\n",
      "['不上', '联系', '酒店'] 61\n",
      "['投屏', '电视', '酒店'] 60\n",
      "['低风险', '地区', '核酸'] 59\n",
      "['低风险', '地区', '需要'] 58\n",
      "['wifi', '密码', '酒店'] 57\n",
      "['低风险', '地区', '星'] 56\n",
      "新挖掘问题数量 45\n",
      "不在标准Q里的数量 44\n",
      "占比 0.9777777777777777\n",
      "45  grams\n",
      "全部处理完毕！\n"
     ]
    }
   ],
   "source": [
    "print('4.2 提取Trigrams：')\n",
    "def get_trigrams_report(df, bigrams, result_df_path, standard_Q_path, time_limit=1000,\n",
    "                        grams_count=200, show_samples=5, occur=0.0001, window=5):\n",
    "\n",
    "    t1=time()\n",
    "    \n",
    "    # input for calculating trigram pmi scores\n",
    "    data_pmi_input = []\n",
    "    for words in df['tokens_list'].tolist():\n",
    "        data_pmi_input.extend(words)\n",
    "        for i in range(window-1):\n",
    "            data_pmi_input.extend('|')\n",
    "    \n",
    "    # calculate trigram pmi scores\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    tri_finder = TrigramCollocationFinder.from_words(data_pmi_input,window_size=window) \n",
    "    \n",
    "    # save trigram pmi scores # generate report\n",
    "    trigrams = []\n",
    "    result_df = pd.DataFrame()\n",
    "    if 0<occur<1:\n",
    "        occur = occur*len(df)\n",
    "    for w1w2w3, score in tri_finder.score_ngrams(trigram_measures.pmi):\n",
    "        t2=time()\n",
    "        if grams_count==0 or t2-t1>time_limit:\n",
    "            break\n",
    "        is_trigram_recorded = False\n",
    "        w1, w2, w3 = w1w2w3\n",
    "        set_w1w2w3 = set([w1,w2,w3])\n",
    "        list_w1w2w3 = sorted(set_w1w2w3)\n",
    "        \n",
    "        for set_w1w2 in bigrams:\n",
    "            if set_w1w2.issubset(set_w1w2w3) and len(set_w1w2w3)==3 and (list_w1w2w3 not in trigrams):\n",
    "                trigrams.append(list_w1w2w3)\n",
    "                is_trigram_recorded=True\n",
    "                break\n",
    "            \n",
    "        if is_trigram_recorded:\n",
    "            temp_df = df['keywords'].apply(set_w1w2w3.issubset)\n",
    "            cnt = temp_df.sum()\n",
    "            row_result = dict()\n",
    "            if cnt>occur:\n",
    "                print(list_w1w2w3, grams_count)\n",
    "                row_result['gram']=w1+'+'+w2+'+'+w3\n",
    "                row_result['pmi']=score\n",
    "                row_result['occurance']=cnt\n",
    "                sample_df = df.iloc[temp_df[temp_df==True].index][:show_samples]\n",
    "                sample_df = sample_df[['session_id','body']]\n",
    "                row_result['samples'] = dict(zip(sample_df.session_id, sample_df.body))\n",
    "                result_df = result_df.append(row_result, ignore_index=True)\n",
    "                grams_count-=1\n",
    "\n",
    "    result_df = add_stardard_Q(result_df, standard_Q_path)\n",
    "    result_df = result_df[['gram', 'occurance', 'pmi', 'standard_Q', 'samples']]\n",
    "    result_df.insert(0, 'org_date_start', datetime.date.today()+timedelta(days=-8))\n",
    "    result_df.insert(0, 'org_date_end', datetime.date.today()+timedelta(days=-2))\n",
    "    result_df.to_csv(result_df_path, header=False, index=False, encoding='utf-8-sig', sep='\\t')\n",
    "    print(len(result_df), ' grams')\n",
    "    return result_df\n",
    "\n",
    "bigrams = result_bigram.gram.tolist()\n",
    "bigrams = [set(x.split('+')) for x in bigrams]\n",
    "\n",
    "result_trigram=get_trigrams_report(\n",
    "    df, bigrams, \n",
    "    result_df_path=trigrams_result_df_path,\n",
    "    standard_Q_path='../data/StandardQ.xlsx', \n",
    "    time_limit=trigrams_time_limit,\n",
    "    grams_count=trigrams_count, \n",
    "    show_samples=trigrams_show_samples, \n",
    "    occur=trigrams_occur, \n",
    "    window=trigrams_window\n",
    ")\n",
    "print('全部处理完毕！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30820, 7)\n",
      "(200, 7)\n",
      "(45, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(result_bigram.shape)\n",
    "print(result_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_date_end</th>\n",
       "      <th>org_date_start</th>\n",
       "      <th>gram</th>\n",
       "      <th>occurance</th>\n",
       "      <th>pmi</th>\n",
       "      <th>standard_Q</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>天+健康+监测</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.691835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{600000090763986: '不用7天健康监测那些吗', 7000000912367...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>支持+居家+隔离</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.660205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{500000090717686: '你好，我想问下支持+7吗好的居家隔离好的没了谢谢你好的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>低风险+地区+核酸</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.129449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{100000091180385: '我问下我从北京过去可以住吧 [愉快]有核酸 低风险地区...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>酒店+电梯+刷卡</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.456062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{100000091108791: '好的这个酒店上电梯要刷卡吗好的，谢谢', 100000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>低风险+地区+星</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.806358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{200000090946361: '江苏省低风险地区持48h核酸检测报告可以入住吗，没有星...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   org_date_end org_date_start       gram  occurance        pmi standard_Q  \\\n",
       "7    2022-04-06     2022-03-31    天+健康+监测       14.0  15.691835        NaN   \n",
       "12   2022-04-06     2022-03-31   支持+居家+隔离       10.0  14.660205        NaN   \n",
       "41   2022-04-06     2022-03-31  低风险+地区+核酸       15.0   9.129449        NaN   \n",
       "24   2022-04-06     2022-03-31   酒店+电梯+刷卡       10.0  12.456062        NaN   \n",
       "44   2022-04-06     2022-03-31   低风险+地区+星        8.0   8.806358        NaN   \n",
       "\n",
       "                                              samples  \n",
       "7   {600000090763986: '不用7天健康监测那些吗', 7000000912367...  \n",
       "12  {500000090717686: '你好，我想问下支持+7吗好的居家隔离好的没了谢谢你好的...  \n",
       "41  {100000091180385: '我问下我从北京过去可以住吧 [愉快]有核酸 低风险地区...  \n",
       "24  {100000091108791: '好的这个酒店上电梯要刷卡吗好的，谢谢', 100000...  \n",
       "44  {200000090946361: '江苏省低风险地区持48h核酸检测报告可以入住吗，没有星...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_trigram.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_date_end</th>\n",
       "      <th>org_date_start</th>\n",
       "      <th>gram</th>\n",
       "      <th>occurance</th>\n",
       "      <th>pmi</th>\n",
       "      <th>standard_Q</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>电脑+配置</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.347762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{100000091258551: '什么配置啊电脑什么配置', 2000000909876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>收到+短信</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.922450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{400000090364769: '好的，谢谢短信还没收到不知道是不是手机设置过拒收陌生短...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>隔音+效果</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.150379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{100000091157429: '隔音效果怎么样', 200000090916056: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>开具+水单</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.211485</td>\n",
       "      <td>我要开具水单</td>\n",
       "      <td>{100000091275277: '可以开具水单吗', 400000090389118: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>厨房+做饭</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.998852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{400000090362497: '有没有厨房可以做饭呀', 40000009038246...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    org_date_end org_date_start   gram  occurance        pmi standard_Q  \\\n",
       "44    2022-04-06     2022-03-31  电脑+配置       13.0   9.347762        NaN   \n",
       "135   2022-04-06     2022-03-31  收到+短信        4.0   7.922450        NaN   \n",
       "5     2022-04-06     2022-03-31  隔音+效果       19.0  11.150379        NaN   \n",
       "17    2022-04-06     2022-03-31  开具+水单        4.0  10.211485     我要开具水单   \n",
       "124   2022-04-06     2022-03-31  厨房+做饭       10.0   7.998852        NaN   \n",
       "\n",
       "                                               samples  \n",
       "44   {100000091258551: '什么配置啊电脑什么配置', 2000000909876...  \n",
       "135  {400000090364769: '好的，谢谢短信还没收到不知道是不是手机设置过拒收陌生短...  \n",
       "5    {100000091157429: '隔音效果怎么样', 200000090916056: ...  \n",
       "17   {100000091275277: '可以开具水单吗', 400000090389118: ...  \n",
       "124  {400000090362497: '有没有厨房可以做饭呀', 40000009038246...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_bigram.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_date_end</th>\n",
       "      <th>org_date_start</th>\n",
       "      <th>session_id</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_list</th>\n",
       "      <th>tokens</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>100000091274542</td>\n",
       "      <td>在吗有房吗多少钱一间能三个人住吗</td>\n",
       "      <td>[房, 钱, 住]</td>\n",
       "      <td>房 钱 住</td>\n",
       "      <td>[钱, 房, 住]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22744</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>600000090856685</td>\n",
       "      <td>您好昨天看到3晚套餐怎么没了？</td>\n",
       "      <td>[看到, 套餐, 没]</td>\n",
       "      <td>看到 套餐 没</td>\n",
       "      <td>[套餐, 看到, 没]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14721</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>400000090447962</td>\n",
       "      <td>电子身份证能不能办理入住？</td>\n",
       "      <td>[电子, 身份证, 办理, 入住]</td>\n",
       "      <td>电子 身份证 办理 入住</td>\n",
       "      <td>[电子, 身份证, 办理]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23678</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>700000091247504</td>\n",
       "      <td>请问一下我从机场过去住 码会黄吗明早还有飞机</td>\n",
       "      <td>[机场, 住, 码会, 黄, 飞机]</td>\n",
       "      <td>机场 住 码会 黄 飞机</td>\n",
       "      <td>[黄, 飞机, 机场, 住]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>200000090943042</td>\n",
       "      <td>费用酒店服务和设施餐饮服务你好餐厅一桌的标准费用是多少呢</td>\n",
       "      <td>[费用, 酒店, 服务, 设施, 餐饮, 服务, 餐厅, 标准, 费用]</td>\n",
       "      <td>费用 酒店 服务 设施 餐饮 服务 餐厅 标准 费用</td>\n",
       "      <td>[费用, 服务, 餐饮, 标准, 餐厅]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      org_date_end org_date_start       session_id  \\\n",
       "3606    2022-04-06     2022-03-31  100000091274542   \n",
       "22744   2022-04-06     2022-03-31  600000090856685   \n",
       "14721   2022-04-06     2022-03-31  400000090447962   \n",
       "23678   2022-04-06     2022-03-31  700000091247504   \n",
       "4290    2022-04-06     2022-03-31  200000090943042   \n",
       "\n",
       "                               body                           tokens_list  \\\n",
       "3606               在吗有房吗多少钱一间能三个人住吗                             [房, 钱, 住]   \n",
       "22744               您好昨天看到3晚套餐怎么没了？                           [看到, 套餐, 没]   \n",
       "14721                 电子身份证能不能办理入住？                     [电子, 身份证, 办理, 入住]   \n",
       "23678        请问一下我从机场过去住 码会黄吗明早还有飞机                    [机场, 住, 码会, 黄, 飞机]   \n",
       "4290   费用酒店服务和设施餐饮服务你好餐厅一桌的标准费用是多少呢  [费用, 酒店, 服务, 设施, 餐饮, 服务, 餐厅, 标准, 费用]   \n",
       "\n",
       "                           tokens              keywords  \n",
       "3606                        房 钱 住             [钱, 房, 住]  \n",
       "22744                     看到 套餐 没           [套餐, 看到, 没]  \n",
       "14721                电子 身份证 办理 入住         [电子, 身份证, 办理]  \n",
       "23678                机场 住 码会 黄 飞机        [黄, 飞机, 机场, 住]  \n",
       "4290   费用 酒店 服务 设施 餐饮 服务 餐厅 标准 费用  [费用, 服务, 餐饮, 标准, 餐厅]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.96534466743469\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "\n",
    "tokenized_tablename = 'tmp_htl_ai_db.tmp_fy_im_weekly_tokenized'\n",
    "bigram_tablename = 'tmp_htl_ai_db.tmp_fy_im_weekly_bigrams'\n",
    "trigram_tablename = 'tmp_htl_ai_db.tmp_fy_im_weekly_trigrams'\n",
    "\n",
    "tokenized_filename = '../temp_data/tokenized.csv'\n",
    "bigrams_result_df_path = '../temp_data/bigram.csv'\n",
    "trigrams_result_df_path = '../temp_data/trigram.csv'\n",
    "\n",
    "# 添加新数据到 hive 临时表\n",
    "os.system(\n",
    " \"\"\"hive -e \"use tmp_htl_ai_db; \n",
    " LOAD DATA LOCAL INPATH '{}' OVERWRITE INTO TABLE {};\" \"\"\".format(tokenized_filename, tokenized_tablename)\n",
    ")\n",
    "os.system(\n",
    " \"\"\"hive -e \"use tmp_htl_ai_db; \n",
    " LOAD DATA LOCAL INPATH '{}' OVERWRITE INTO TABLE {};\" \"\"\".format(bigrams_result_df_path, bigram_tablename)\n",
    ")\n",
    "os.system(\n",
    " \"\"\"hive -e \"use tmp_htl_ai_db; \n",
    " LOAD DATA LOCAL INPATH '{}' OVERWRITE INTO TABLE {};\" \"\"\".format(trigrams_result_df_path, trigram_tablename)\n",
    ")\n",
    "\n",
    "t2 = time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "临时表数据写入正式表：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "today = date.today()\n",
    "d = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "adm_tokenized_tablename = 'htl_ai_db.adm_fy_im_weekly_tokenized'\n",
    "adm_bigram_tablename = 'htl_ai_db.adm_fy_im_weekly_bigrams'\n",
    "adm_trigram_tablename = 'htl_ai_db.adm_fy_im_weekly_trigrams'\n",
    "\n",
    "print('临时表数据写入正式表：')\n",
    "\n",
    "# os.system(\n",
    "#  \"\"\"hive -e \"\n",
    "#     insert into table htl_ai_db.adm_fy_im_weekly_trigrams partition(d='2022-04-15')\n",
    "#     select * from tmp_htl_ai_db.tmp_fy_im_weekly_trigrams;\n",
    "#  \" \"\"\"\n",
    "# )\n",
    "\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    "    insert into table {} partition(d='{}')\n",
    "    select * from {};\n",
    " \" \"\"\".format(adm_bigram_tablename, d, bigram_tablename)\n",
    ")\n",
    "\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    "    insert into table {} partition(d='{}')\n",
    "    select * from {};\n",
    " \" \"\"\".format(adm_trigram_tablename, d, trigram_tablename)\n",
    ")\n",
    "os.system(\n",
    " \"\"\"hive -e \"\n",
    "    insert into table {} partition(d='{}')\n",
    "    select * from {};\n",
    " \" \"\"\".format(adm_tokenized_tablename, d, tokenized_tablename)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.5 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
